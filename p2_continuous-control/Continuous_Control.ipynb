{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ddpg_agent\n",
    "# env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    False\n",
    "# state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# # for t in range(\n",
    "# total_steps = 0\n",
    "# while True:\n",
    "#     total_steps += 1\n",
    "#     action = agent.act(state, eps)\n",
    "#     env_start = timeit.default_timer()\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     total_env_time += timeit.default_timer() - env_start\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     agent.step(state, action, reward, next_state, done)\n",
    "#     state = next_state\n",
    "#     score += reward\n",
    "#     if done:\n",
    "#         break \n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m<ipython-input-10-2e4a8bc04803>\u001b[0m(7)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      5 \u001b[0;31m\u001b[0;31m# %autoreload 2b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      6 \u001b[0;31m\u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 7 \u001b[0;31m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8 \u001b[0;31m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      9 \u001b[0;31m\u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b ddpg_agent.py:72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoint 1 at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:72\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(72)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     70 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     71 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m1\u001b[0;32m--> 72 \u001b[0;31m                \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     73 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(73)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     71 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m1\u001b[0;32m    72 \u001b[0;31m                \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 73 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     75 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  experiences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 3.1671, -2.4554,  0.0806,  ...,  1.0000,  0.0000, -0.5330],\n",
      "        [ 1.0665, -3.7361, -0.9581,  ...,  1.0000,  0.0000, -0.5330],\n",
      "        [ 3.0599,  0.2080,  2.6059,  ...,  1.0000,  0.0000, -0.5330],\n",
      "        ...,\n",
      "        [ 1.4406, -3.5766, -1.0735,  ...,  1.0000,  0.0000, -0.5330],\n",
      "        [ 3.2900, -2.2858,  0.1483,  ...,  1.0000,  0.0000, -0.5330],\n",
      "        [ 0.0529, -3.9996, -0.0236,  ...,  1.0000,  0.0000, -0.5330]]), tensor([[ 0.4450,  0.3527, -0.7196,  0.1385],\n",
      "        [-0.3562,  0.4302, -0.3297, -0.1066],\n",
      "        [-0.3164, -0.0193, -0.1500, -0.2409],\n",
      "        [ 0.2616, -0.0708, -0.3062,  0.0624],\n",
      "        [ 0.6537, -0.1507,  0.9100,  0.1443],\n",
      "        [ 0.2219,  0.5544,  0.1349, -0.1559],\n",
      "        [-0.0358,  0.0632,  0.5828, -0.1998],\n",
      "        [-0.4480, -0.3018,  0.1283, -0.4996],\n",
      "        [ 0.7071,  0.0266, -1.0000,  0.2297],\n",
      "        [ 0.0276, -0.4318,  0.1052,  0.0431],\n",
      "        [ 0.1078,  0.4386, -0.0437,  0.2469],\n",
      "        [ 0.1496,  0.4152, -0.2396,  0.1752],\n",
      "        [ 0.2478, -0.1185, -0.1801, -0.1625],\n",
      "        [ 0.2327,  0.4608, -0.1829,  0.1332],\n",
      "        [-0.0198,  0.0397, -0.3468, -0.2259],\n",
      "        [ 0.2217,  0.1134,  0.7598, -0.0859],\n",
      "        [ 0.6560,  0.0780, -0.4029, -0.0330],\n",
      "        [-0.2984, -0.1694, -0.0502, -0.0786],\n",
      "        [-0.1189, -0.6838, -0.5379, -0.2375],\n",
      "        [ 0.1570, -0.5801, -0.1705, -0.1236],\n",
      "        [-0.1306,  0.4265, -0.0373,  0.2333],\n",
      "        [-0.6152,  0.0801,  0.0500, -0.3640],\n",
      "        [ 0.1114, -0.3138, -0.2727,  0.2797],\n",
      "        [ 0.3272, -0.6918,  0.3628, -0.5852],\n",
      "        [ 0.1873,  0.2445, -0.1361, -0.0488],\n",
      "        [ 0.0133, -0.4690, -0.3101, -0.1571],\n",
      "        [ 0.1848,  0.1826,  0.3764, -0.1802],\n",
      "        [ 0.2421,  0.2413, -0.5716,  0.0869],\n",
      "        [ 0.0548,  0.1706, -0.0425, -0.0682],\n",
      "        [ 0.3909, -0.3618,  0.7061,  0.0327],\n",
      "        [-0.6090, -0.4930, -0.0917, -0.1425],\n",
      "        [-0.5353,  0.1488,  0.0893, -0.3012],\n",
      "        [ 0.2879, -0.3879,  0.1978, -0.1527],\n",
      "        [ 0.1601, -0.1811, -0.1029,  0.0235],\n",
      "        [ 0.0576,  0.0541,  0.4994, -0.2054],\n",
      "        [-0.0218,  0.0890,  0.0513, -0.3471],\n",
      "        [-0.4660, -0.4778, -0.1135, -0.0258],\n",
      "        [-0.2447, -0.0746,  0.0508, -0.4376],\n",
      "        [-0.0336, -0.8483, -0.3535, -0.3539],\n",
      "        [ 0.1986, -0.2275, -0.2537, -0.1501],\n",
      "        [ 0.0469, -0.5432,  0.1539, -0.0059],\n",
      "        [-0.3634, -0.6364, -0.0642, -0.3443],\n",
      "        [ 0.0446,  0.3620, -0.2962,  0.1193],\n",
      "        [-0.2489,  0.7859, -0.0201, -0.0838],\n",
      "        [-0.0414, -0.1479,  0.0800, -0.1929],\n",
      "        [ 0.5202,  0.3679, -0.8732, -0.0357],\n",
      "        [ 0.5775,  0.0075, -0.9378,  0.0500],\n",
      "        [ 0.1211, -0.5193,  0.6486, -0.5518],\n",
      "        [-0.5832,  0.6035, -0.2870, -0.0522],\n",
      "        [-0.1364, -0.3007,  0.2831,  0.0474],\n",
      "        [-0.5255, -0.2588,  0.1248, -0.3523],\n",
      "        [ 0.6668, -0.5689,  0.6474, -0.1605],\n",
      "        [-0.0040,  0.7309, -0.4531, -0.0199],\n",
      "        [-0.0886,  0.0565, -0.4608,  0.1210],\n",
      "        [-0.2374, -0.4322,  0.0494, -0.3499],\n",
      "        [-0.5573,  0.3890,  0.1048, -0.3436],\n",
      "        [ 0.3251,  0.0736, -0.7855, -0.1377],\n",
      "        [ 0.2653, -0.0294, -0.3705,  0.1704],\n",
      "        [ 0.1835,  0.3914, -0.5681,  0.0100],\n",
      "        [-0.1573, -0.6164,  0.5757, -0.8021],\n",
      "        [-0.7308, -0.0394, -0.3713, -0.3515],\n",
      "        [-0.3316,  0.5078, -0.0305,  0.1103],\n",
      "        [ 0.2681,  0.2610, -0.5554,  0.2605],\n",
      "        [ 0.0499,  0.5375, -0.2641,  0.0579]]), tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[ 3.2900, -2.2858,  0.1483,  ...,  1.0000,  0.0000, -0.5330],\n",
      "        [ 1.2387, -3.6563, -1.0552,  ...,  1.0000,  0.0000, -0.5330],\n",
      "        [ 3.2406,  0.7554,  2.2683,  ...,  1.0000,  0.0000, -0.5330],\n",
      "        ...,\n",
      "        [ 1.6781, -3.4904, -1.0130,  ...,  1.0000,  0.0000, -0.5330],\n",
      "        [ 3.3952, -2.1259,  0.1753,  ...,  1.0000,  0.0000, -0.5330],\n",
      "        [ 0.1498, -3.9958, -0.1132,  ...,  1.0000,  0.0000, -0.5330]]), tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  shape experiences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SyntaxError: invalid syntax\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  experiences.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** AttributeError: 'tuple' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  np.shape(experiences)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** ValueError: only one element tensors can be converted to Python scalars\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  type(experiences)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m     68 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     69 \u001b[0m            \u001b[0;31m# Learn, if enough samples are available in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     70 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     71 \u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m1\u001b[1;32m    72 \u001b[0m                \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 73 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     74 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     75 \u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     76 \u001b[0m        \u001b[0;34m\"\"\"Returns actions for given state as per current policy.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     77 \u001b[0m        \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     78 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(90)\u001b[0;36mlearn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     88 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     89 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 90 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     91 \u001b[0;31m        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
      "\u001b[0m\u001b[0;32m     92 \u001b[0;31m        \u001b[0mQ_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mÎ³\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcritic_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(102)\u001b[0;36mlearn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    100 \u001b[0;31m            \u001b[0mgamma\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    101 \u001b[0;31m        \"\"\"\n",
      "\u001b[0m\u001b[0;32m--> 102 \u001b[0;31m        \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    103 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    104 \u001b[0;31m        \u001b[0;31m# ---------------------------- update critic ---------------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print lines of code from the current stack frame\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m     97 \u001b[0m        \u001b[0mParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     98 \u001b[0m        \u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     99 \u001b[0m            \u001b[0mexperiences\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    100 \u001b[0m            \u001b[0mgamma\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    101 \u001b[0m        \"\"\"\n",
      "\u001b[0;32m--> 102 \u001b[0;31m        \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    103 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    104 \u001b[0m        \u001b[0;31m# ---------------------------- update critic ---------------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    105 \u001b[0m        \u001b[0;31m# Get predicted next-state actions and Q values from target models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    106 \u001b[0m        \u001b[0mactions_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    107 \u001b[0m        \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(106)\u001b[0;36mlearn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    104 \u001b[0;31m        \u001b[0;31m# ---------------------------- update critic ---------------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    105 \u001b[0;31m        \u001b[0;31m# Get predicted next-state actions and Q values from target models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 106 \u001b[0;31m        \u001b[0mactions_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    107 \u001b[0;31m        \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    108 \u001b[0;31m        \u001b[0;31m# Compute Q targets for current states (y_i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  np.shape(states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 33])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  np.shape(actions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  np.shape(rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b act\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** The specified object 'act' is not a function or was not found along sys.path.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b Agent.act\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoint 2 at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:75\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ddpg_agent.ReplayBuffer object at 0x7fb4ed990518>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.memory.memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([Experience(state=array([ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "       -0.00000000e+00, -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00, -1.00000000e+01,  0.00000000e+00,\n",
      "        1.00000000e+00, -0.00000000e+00, -0.00000000e+00, -4.37113883e-08,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00, -6.30408478e+00, -1.00000000e+00,\n",
      "       -4.92529202e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.13057055,  0.42653787, -0.03732488,  0.23326162], dtype=float32), reward=[0.0], next_state=array([ 1.06143951e-02, -3.99998260e+00,  7.24867499e-03,  9.99998748e-01,\n",
      "        1.32018444e-03, -1.00042257e-06,  9.01577994e-04, -3.59757915e-02,\n",
      "        5.71791061e-05,  5.27655929e-02,  2.12117672e-01,  4.92979190e-04,\n",
      "        1.44622102e-01,  2.57720947e-02, -9.99996185e+00,  8.35843850e-03,\n",
      "        9.99999106e-01,  1.20617892e-03, -2.87504690e-06, -7.16612791e-04,\n",
      "        2.85254922e-02,  1.63642791e-04,  4.82628606e-02,  5.85402846e-01,\n",
      "        1.28167751e-03,  1.25479847e-01, -6.11648560e+00, -1.00000000e+00,\n",
      "       -5.15641546e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 1.06143951e-02, -3.99998260e+00,  7.24867499e-03,  9.99998748e-01,\n",
      "        1.32018444e-03, -1.00042257e-06,  9.01577994e-04, -3.59757915e-02,\n",
      "        5.71791061e-05,  5.27655929e-02,  2.12117672e-01,  4.92979190e-04,\n",
      "        1.44622102e-01,  2.57720947e-02, -9.99996185e+00,  8.35843850e-03,\n",
      "        9.99999106e-01,  1.20617892e-03, -2.87504690e-06, -7.16612791e-04,\n",
      "        2.85254922e-02,  1.63642791e-04,  4.82628606e-02,  5.85402846e-01,\n",
      "        1.28167751e-03,  1.25479847e-01, -6.11648560e+00, -1.00000000e+00,\n",
      "       -5.15641546e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.232655  ,  0.46077213, -0.18292563,  0.13320702], dtype=float32), reward=[0.0], next_state=array([ 5.28640747e-02, -3.99963284e+00, -2.36029066e-02,  9.99974072e-01,\n",
      "        6.57527009e-03,  1.61954322e-05, -2.93579930e-03,  1.74528793e-01,\n",
      "       -4.24209546e-04,  1.78137094e-01,  7.16085553e-01,  8.53681192e-03,\n",
      "       -7.01560318e-01,  9.62886810e-02, -9.99913502e+00,  3.25687230e-03,\n",
      "        9.99972343e-01,  6.62627514e-04,  1.12708345e-04,  7.41210300e-03,\n",
      "       -3.41245830e-01, -7.20761809e-03, -5.01140691e-02,  1.02698541e+00,\n",
      "        2.65503060e-02,  2.95035064e-01, -5.92041588e+00, -1.00000000e+00,\n",
      "       -5.38039780e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 5.28640747e-02, -3.99963284e+00, -2.36029066e-02,  9.99974072e-01,\n",
      "        6.57527009e-03,  1.61954322e-05, -2.93579930e-03,  1.74528793e-01,\n",
      "       -4.24209546e-04,  1.78137094e-01,  7.16085553e-01,  8.53681192e-03,\n",
      "       -7.01560318e-01,  9.62886810e-02, -9.99913502e+00,  3.25687230e-03,\n",
      "        9.99972343e-01,  6.62627514e-04,  1.12708345e-04,  7.41210300e-03,\n",
      "       -3.41245830e-01, -7.20761809e-03, -5.01140691e-02,  1.02698541e+00,\n",
      "        2.65503060e-02,  2.95035064e-01, -5.92041588e+00, -1.00000000e+00,\n",
      "       -5.38039780e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.04990006,  0.5375219 , -0.26412332,  0.05788818], dtype=float32), reward=[0.0], next_state=array([ 1.49847031e-01, -3.99581480e+00, -1.13230079e-01,  9.99727011e-01,\n",
      "        1.86387021e-02,  2.50030716e-04, -1.40907010e-02,  3.39305043e-01,\n",
      "       -8.01212247e-03,  3.74154001e-01,  1.50383461e+00,  7.40135387e-02,\n",
      "       -1.36218178e+00,  2.18963623e-01, -9.98755550e+00, -1.91102102e-02,\n",
      "        9.99529719e-01, -7.07505178e-03,  1.74424634e-03,  2.97881328e-02,\n",
      "       -6.86516166e-01, -6.85109273e-02, -2.75946915e-01,  1.38404882e+00,\n",
      "        2.80130833e-01,  6.73779964e-01, -5.71615028e+00, -1.00000000e+00,\n",
      "       -5.59693050e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 1.49847031e-01, -3.99581480e+00, -1.13230079e-01,  9.99727011e-01,\n",
      "        1.86387021e-02,  2.50030716e-04, -1.40907010e-02,  3.39305043e-01,\n",
      "       -8.01212247e-03,  3.74154001e-01,  1.50383461e+00,  7.40135387e-02,\n",
      "       -1.36218178e+00,  2.18963623e-01, -9.98755550e+00, -1.91102102e-02,\n",
      "        9.99529719e-01, -7.07505178e-03,  1.74424634e-03,  2.97881328e-02,\n",
      "       -6.86516166e-01, -6.85109273e-02, -2.75946915e-01,  1.38404882e+00,\n",
      "        2.80130833e-01,  6.73779964e-01, -5.71615028e+00, -1.00000000e+00,\n",
      "       -5.59693050e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.00401499,  0.7309314 , -0.45312795, -0.01985111], dtype=float32), reward=[0.0], next_state=array([ 3.30278397e-01, -3.97830391e+00, -2.63896585e-01,  9.98612344e-01,\n",
      "        4.10890989e-02,  1.31820468e-03, -3.29151489e-02,  5.44603884e-01,\n",
      "       -3.68834026e-02,  6.68895602e-01,  2.68698335e+00,  3.05404067e-01,\n",
      "       -2.17085814e+00,  4.07526016e-01, -9.93014145e+00, -4.65092510e-02,\n",
      "        9.97151971e-01, -2.76225843e-02,  1.00227511e-02,  6.94591329e-02,\n",
      "       -1.16977513e+00, -3.07820618e-01, -6.37781918e-01,  1.71341097e+00,\n",
      "        1.25653458e+00,  1.32327378e+00, -5.50396729e+00, -1.00000000e+00,\n",
      "       -5.80571556e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 3.30278397e-01, -3.97830391e+00, -2.63896585e-01,  9.98612344e-01,\n",
      "        4.10890989e-02,  1.31820468e-03, -3.29151489e-02,  5.44603884e-01,\n",
      "       -3.68834026e-02,  6.68895602e-01,  2.68698335e+00,  3.05404067e-01,\n",
      "       -2.17085814e+00,  4.07526016e-01, -9.93014145e+00, -4.65092510e-02,\n",
      "        9.97151971e-01, -2.76225843e-02,  1.00227511e-02,  6.94591329e-02,\n",
      "       -1.16977513e+00, -3.07820618e-01, -6.37781918e-01,  1.71341097e+00,\n",
      "        1.25653458e+00,  1.32327378e+00, -5.50396729e+00, -1.00000000e+00,\n",
      "       -5.80571556e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.08842065,  0.26785144, -0.3616401 , -0.04979403], dtype=float32), reward=[0.0], next_state=array([ 5.54273605e-01, -3.93770552e+00, -4.41707790e-01,  9.96070266e-01,\n",
      "        6.89927489e-02,  3.79667757e-03, -5.54036945e-02,  5.79776466e-01,\n",
      "       -7.15902299e-02,  7.15551674e-01,  2.86914301e+00,  5.86240768e-01,\n",
      "       -2.26607370e+00,  6.29384995e-01, -9.78843212e+00, -6.16595447e-02,\n",
      "        9.90763366e-01, -5.32935597e-02,  3.06152888e-02,  1.20873645e-01,\n",
      "       -1.38269973e+00, -6.10233963e-01, -6.15741849e-01,  1.76710546e+00,\n",
      "        2.59444046e+00,  1.76846278e+00, -5.28416634e+00, -1.00000000e+00,\n",
      "       -6.00646210e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 5.54273605e-01, -3.93770552e+00, -4.41707790e-01,  9.96070266e-01,\n",
      "        6.89927489e-02,  3.79667757e-03, -5.54036945e-02,  5.79776466e-01,\n",
      "       -7.15902299e-02,  7.15551674e-01,  2.86914301e+00,  5.86240768e-01,\n",
      "       -2.26607370e+00,  6.29384995e-01, -9.78843212e+00, -6.16595447e-02,\n",
      "        9.90763366e-01, -5.32935597e-02,  3.06152888e-02,  1.20873645e-01,\n",
      "       -1.38269973e+00, -6.10233963e-01, -6.15741849e-01,  1.76710546e+00,\n",
      "        2.59444046e+00,  1.76846278e+00, -5.28416634e+00, -1.00000000e+00,\n",
      "       -6.00646210e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.08861673,  0.05649568, -0.46077332,  0.12104496], dtype=float32), reward=[0.0], next_state=array([ 7.47726440e-01, -3.88049269e+00, -6.25879049e-01,  9.92480338e-01,\n",
      "        9.31104198e-02,  7.38831749e-03, -7.91117325e-02,  6.22666538e-01,\n",
      "       -8.08330253e-02,  5.46434760e-01,  2.18652034e+00,  7.45723188e-01,\n",
      "       -2.38124275e+00,  8.42220306e-01, -9.56964779e+00, -5.39467931e-02,\n",
      "        9.79270160e-01, -6.77491501e-02,  6.10837638e-02,  1.80855751e-01,\n",
      "       -1.66265368e+00, -7.32544184e-01, -1.91652462e-01,  1.98110485e+00,\n",
      "        3.78008747e+00,  2.30661416e+00, -5.05704880e+00, -1.00000000e+00,\n",
      "       -6.19889116e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 7.47726440e-01, -3.88049269e+00, -6.25879049e-01,  9.92480338e-01,\n",
      "        9.31104198e-02,  7.38831749e-03, -7.91117325e-02,  6.22666538e-01,\n",
      "       -8.08330253e-02,  5.46434760e-01,  2.18652034e+00,  7.45723188e-01,\n",
      "       -2.38124275e+00,  8.42220306e-01, -9.56964779e+00, -5.39467931e-02,\n",
      "        9.79270160e-01, -6.77491501e-02,  6.10837638e-02,  1.80855751e-01,\n",
      "       -1.66265368e+00, -7.32544184e-01, -1.91652462e-01,  1.98110485e+00,\n",
      "        3.78008747e+00,  2.30661416e+00, -5.05704880e+00, -1.00000000e+00,\n",
      "       -6.19889116e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.14963754,  0.41522503, -0.23955593,  0.17518674], dtype=float32), reward=[0.0], next_state=array([ 0.91178513, -3.8123579 , -0.80363727,  0.98816401,  0.1135401 ,\n",
      "        0.01174534, -0.10248286,  0.59396183, -0.09955789,  0.50395501,\n",
      "        2.01615047,  0.89126402, -2.20016432,  1.05836487, -9.28914738,\n",
      "       -0.03215843,  0.96262205, -0.06833808,  0.09573898,  0.24397293,\n",
      "       -1.70506895, -0.76650113,  0.10099097,  2.45174694,  4.74319124,\n",
      "        2.30083346, -4.82292938, -1.        , -6.38273859,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 0.91178513, -3.8123579 , -0.80363727,  0.98816401,  0.1135401 ,\n",
      "        0.01174534, -0.10248286,  0.59396183, -0.09955789,  0.50395501,\n",
      "        2.01615047,  0.89126402, -2.20016432,  1.05836487, -9.28914738,\n",
      "       -0.03215843,  0.96262205, -0.06833808,  0.09573898,  0.24397293,\n",
      "       -1.70506895, -0.76650113,  0.10099097,  2.45174694,  4.74319124,\n",
      "        2.30083346, -4.82292938, -1.        , -6.38273859,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.04461012,  0.36195022, -0.2962478 ,  0.11933719], dtype=float32), reward=[0.0], next_state=array([ 1.06647110e+00, -3.73607802e+00, -9.58145976e-01,  9.83286202e-01,\n",
      "        1.32804260e-01,  1.66448280e-02, -1.23427354e-01,  5.21308959e-01,\n",
      "       -1.17462158e-01,  4.80941504e-01,  1.92420650e+00,  9.75341141e-01,\n",
      "       -1.84750223e+00,  1.28941154e+00, -8.96044540e+00,  6.48808479e-03,\n",
      "        9.41344619e-01, -5.91682680e-02,  1.30868852e-01,  3.05356950e-01,\n",
      "       -1.70012152e+00, -7.16230154e-01,  3.04880053e-01,  2.77284908e+00,\n",
      "        5.49794483e+00,  2.41442084e+00, -4.58213043e+00, -1.00000000e+00,\n",
      "       -6.55774927e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 1.06647110e+00, -3.73607802e+00, -9.58145976e-01,  9.83286202e-01,\n",
      "        1.32804260e-01,  1.66448280e-02, -1.23427354e-01,  5.21308959e-01,\n",
      "       -1.17462158e-01,  4.80941504e-01,  1.92420650e+00,  9.75341141e-01,\n",
      "       -1.84750223e+00,  1.28941154e+00, -8.96044540e+00,  6.48808479e-03,\n",
      "        9.41344619e-01, -5.91682680e-02,  1.30868852e-01,  3.05356950e-01,\n",
      "       -1.70012152e+00, -7.16230154e-01,  3.04880053e-01,  2.77284908e+00,\n",
      "        5.49794483e+00,  2.41442084e+00, -4.58213043e+00, -1.00000000e+00,\n",
      "       -6.55774927e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.35616282,  0.4301874 , -0.32966977, -0.10660212], dtype=float32), reward=[0.0], next_state=array([ 1.2387352 , -3.65631771, -1.05519807,  0.97813255,  0.15445098,\n",
      "        0.02170791, -0.13758823,  0.30047092, -0.16266099,  0.57943368,\n",
      "        2.31064177,  1.00293911, -0.91665655,  1.5353241 , -8.60434818,\n",
      "        0.09541881,  0.91605788, -0.04837161,  0.16748366,  0.36117512,\n",
      "       -1.55947447, -0.83710623,  0.30604276,  2.72660375,  5.82384968,\n",
      "        2.87241149, -4.33499146, -1.        , -6.72367859,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 1.2387352 , -3.65631771, -1.05519807,  0.97813255,  0.15445098,\n",
      "        0.02170791, -0.13758823,  0.30047092, -0.16266099,  0.57943368,\n",
      "        2.31064177,  1.00293911, -0.91665655,  1.5353241 , -8.60434818,\n",
      "        0.09541881,  0.91605788, -0.04837161,  0.16748366,  0.36117512,\n",
      "       -1.55947447, -0.83710623,  0.30604276,  2.72660375,  5.82384968,\n",
      "        2.87241149, -4.33499146, -1.        , -6.72367859,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.58322334,  0.60354596, -0.28700396, -0.05221462], dtype=float32), reward=[0.0], next_state=array([ 1.44062614, -3.57663608, -1.07348454,  0.97293693,  0.18030278,\n",
      "        0.02633016, -0.14209689,  0.05032159, -0.19942723,  0.67019993,\n",
      "        2.63835382,  0.98442274,  0.09482926,  1.79607773, -8.24503803,\n",
      "        0.25912821,  0.8873477 , -0.03935022,  0.20788252,  0.40969554,\n",
      "       -1.41689253, -0.94999444,  0.33942962,  2.68697596,  5.76564598,\n",
      "        3.47614074, -4.08184624, -1.        , -6.88030005,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 1.44062614, -3.57663608, -1.07348454,  0.97293693,  0.18030278,\n",
      "        0.02633016, -0.14209689,  0.05032159, -0.19942723,  0.67019993,\n",
      "        2.63835382,  0.98442274,  0.09482926,  1.79607773, -8.24503803,\n",
      "        0.25912821,  0.8873477 , -0.03935022,  0.20788252,  0.40969554,\n",
      "       -1.41689253, -0.94999444,  0.33942962,  2.68697596,  5.76564598,\n",
      "        3.47614074, -4.08184624, -1.        , -6.88030005,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.3315902 ,  0.50783116, -0.03054088,  0.11028144], dtype=float32), reward=[0.0], next_state=array([ 1.67814636, -3.49043369, -1.01298153,  0.96729243,  0.21151766,\n",
      "        0.02992903, -0.13678433, -0.19795585, -0.24299082,  0.82910764,\n",
      "        3.18121338,  1.133834  ,  1.09183788,  2.0847168 , -7.89967442,\n",
      "        0.48669779,  0.85672164, -0.03209525,  0.25349149,  0.44804034,\n",
      "       -1.15455341, -1.1378783 ,  0.42561793,  2.94260836,  5.29414177,\n",
      "        3.68087459, -3.82305145, -1.        , -7.02739525,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 1.67814636, -3.49043369, -1.01298153,  0.96729243,  0.21151766,\n",
      "        0.02992903, -0.13678433, -0.19795585, -0.24299082,  0.82910764,\n",
      "        3.18121338,  1.133834  ,  1.09183788,  2.0847168 , -7.89967442,\n",
      "        0.48669779,  0.85672164, -0.03209525,  0.25349149,  0.44804034,\n",
      "       -1.15455341, -1.1378783 ,  0.42561793,  2.94260836,  5.29414177,\n",
      "        3.68087459, -3.82305145, -1.        , -7.02739525,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.24885201,  0.7858749 , -0.0200657 , -0.08377612], dtype=float32), reward=[0.0], next_state=array([ 1.95948029, -3.38045526, -0.87514752,  0.96011567,  0.24977085,\n",
      "        0.03168575, -0.12160754, -0.44772691, -0.27060345,  1.01248169,\n",
      "        3.72196794,  1.49858475,  2.04640484,  2.40617561, -7.56530571,\n",
      "        0.76519573,  0.82436514, -0.02816301,  0.30825317,  0.47392932,\n",
      "       -0.89181978, -1.42164767,  0.54838395,  3.02755713,  4.86194658,\n",
      "        3.74860072, -3.55896378, -1.        , -7.16475964,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 1.95948029, -3.38045526, -0.87514752,  0.96011567,  0.24977085,\n",
      "        0.03168575, -0.12160754, -0.44772691, -0.27060345,  1.01248169,\n",
      "        3.72196794,  1.49858475,  2.04640484,  2.40617561, -7.56530571,\n",
      "        0.76519573,  0.82436514, -0.02816301,  0.30825317,  0.47392932,\n",
      "       -0.89181978, -1.42164767,  0.54838395,  3.02755713,  4.86194658,\n",
      "        3.74860072, -3.55896378, -1.        , -7.16475964,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.00833109,  0.3410196 , -0.18718506,  0.02655105], dtype=float32), reward=[0.0], next_state=array([ 2.25010681, -3.24096251, -0.6874429 ,  0.9510715 ,  0.29101887,\n",
      "        0.0304294 , -0.09922281, -0.59451538, -0.23341438,  1.0433706 ,\n",
      "        3.61363244,  1.83455455,  2.46946907,  2.73648643, -7.22419024,\n",
      "        1.07534409,  0.78946501, -0.02320244,  0.36758173,  0.49100956,\n",
      "       -0.8245452 , -1.42542398,  0.68595773,  3.07795477,  4.82745504,\n",
      "        3.80245924, -3.28994751, -1.        , -7.29220438,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 2.25010681, -3.24096251, -0.6874429 ,  0.9510715 ,  0.29101887,\n",
      "        0.0304294 , -0.09922281, -0.59451538, -0.23341438,  1.0433706 ,\n",
      "        3.61363244,  1.83455455,  2.46946907,  2.73648643, -7.22419024,\n",
      "        1.07534409,  0.78946501, -0.02320244,  0.36758173,  0.49100956,\n",
      "       -0.8245452 , -1.42542398,  0.68595773,  3.07795477,  4.82745504,\n",
      "        3.80245924, -3.28994751, -1.        , -7.29220438,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.10779905,  0.4385719 , -0.04374782,  0.24689683], dtype=float32), reward=[0.0], next_state=array([ 2.539814  , -3.06248903, -0.46798289,  0.93946493,  0.33424422,\n",
      "        0.02536197, -0.07101631, -0.76351273, -0.19154325,  1.13933837,\n",
      "        3.66868281,  2.40971327,  2.86363554,  3.07933235, -6.86780405,\n",
      "        1.39631093,  0.75258058, -0.01542637,  0.42961302,  0.49881566,\n",
      "       -0.65380162, -1.49344981,  0.8849532 ,  3.43774295,  4.64594221,\n",
      "        3.54750824, -3.0163765 , -1.        , -7.40955305,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 2.539814  , -3.06248903, -0.46798289,  0.93946493,  0.33424422,\n",
      "        0.02536197, -0.07101631, -0.76351273, -0.19154325,  1.13933837,\n",
      "        3.66868281,  2.40971327,  2.86363554,  3.07933235, -6.86780405,\n",
      "        1.39631093,  0.75258058, -0.01542637,  0.42961302,  0.49881566,\n",
      "       -0.65380162, -1.49344981,  0.8849532 ,  3.43774295,  4.64594221,\n",
      "        3.54750824, -3.0163765 , -1.        , -7.40955305,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.18734781,  0.2444853 , -0.1361418 , -0.04883387], dtype=float32), reward=[0.0], next_state=array([ 2.80218506e+00, -2.85382509e+00, -2.42840022e-01,  9.25769210e-01,\n",
      "        3.75714064e-01,  1.59952790e-02, -3.91730666e-02, -8.35133135e-01,\n",
      "       -1.07502669e-01,  1.05314851e+00,  3.11506796e+00,  2.63769269e+00,\n",
      "        2.73945665e+00,  3.41256905e+00, -6.50222826e+00,  1.70845747e+00,\n",
      "        7.15148807e-01, -3.49868415e-03,  4.89164352e-01,  4.99267727e-01,\n",
      "       -6.18213832e-01, -1.30594730e+00,  9.56222355e-01,  3.39853191e+00,\n",
      "        4.51692057e+00,  3.34315085e+00, -2.73862839e+00, -1.00000000e+00,\n",
      "       -7.51664305e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 2.80218506e+00, -2.85382509e+00, -2.42840022e-01,  9.25769210e-01,\n",
      "        3.75714064e-01,  1.59952790e-02, -3.91730666e-02, -8.35133135e-01,\n",
      "       -1.07502669e-01,  1.05314851e+00,  3.11506796e+00,  2.63769269e+00,\n",
      "        2.73945665e+00,  3.41256905e+00, -6.50222826e+00,  1.70845747e+00,\n",
      "        7.15148807e-01, -3.49868415e-03,  4.89164352e-01,  4.99267727e-01,\n",
      "       -6.18213832e-01, -1.30594730e+00,  9.56222355e-01,  3.39853191e+00,\n",
      "        4.51692057e+00,  3.34315085e+00, -2.73862839e+00, -1.00000000e+00,\n",
      "       -7.51664305e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.2616008 , -0.0707804 , -0.30620587,  0.06240675], dtype=float32), reward=[0.0], next_state=array([ 3.00865555e+00, -2.64679217e+00, -5.48647791e-02,  9.11912680e-01,\n",
      "        4.10251677e-01,  4.33850801e-03, -9.48627386e-03, -7.37226963e-01,\n",
      "       -3.08992565e-02,  8.59877944e-01,  2.33922410e+00,  2.47604322e+00,\n",
      "        2.09453797e+00,  3.70496178e+00, -6.13501835e+00,  1.99148655e+00,\n",
      "        6.79041803e-01,  1.54438168e-02,  5.37234068e-01,  5.00043392e-01,\n",
      "       -7.54637659e-01, -9.28999722e-01,  8.73928010e-01,  3.14667058e+00,\n",
      "        4.67485571e+00,  3.22114539e+00, -2.45708847e+00, -1.00000000e+00,\n",
      "       -7.61332464e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 3.00865555e+00, -2.64679217e+00, -5.48647791e-02,  9.11912680e-01,\n",
      "        4.10251677e-01,  4.33850801e-03, -9.48627386e-03, -7.37226963e-01,\n",
      "       -3.08992565e-02,  8.59877944e-01,  2.33922410e+00,  2.47604322e+00,\n",
      "        2.09453797e+00,  3.70496178e+00, -6.13501835e+00,  1.99148655e+00,\n",
      "        6.79041803e-01,  1.54438168e-02,  5.37234068e-01,  5.00043392e-01,\n",
      "       -7.54637659e-01, -9.28999722e-01,  8.73928010e-01,  3.14667058e+00,\n",
      "        4.67485571e+00,  3.22114539e+00, -2.45708847e+00, -1.00000000e+00,\n",
      "       -7.61332464e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.26527292, -0.02935868, -0.37052232,  0.17037609], dtype=float32), reward=[0.0], next_state=array([ 3.16712379e+00, -2.45537901e+00,  8.06134641e-02,  8.98765922e-01,\n",
      "        4.38130826e-01, -7.04387715e-03,  1.45492740e-02, -5.94636202e-01,\n",
      "        1.45733533e-02,  7.21471548e-01,  1.81999481e+00,  2.28969479e+00,\n",
      "        1.45378733e+00,  3.95569229e+00, -5.76167154e+00,  2.23629761e+00,\n",
      "        6.44852638e-01,  4.10454161e-02,  5.72122514e-01,  5.05129814e-01,\n",
      "       -9.01470959e-01, -6.30758345e-01,  7.56565630e-01,  2.87593865e+00,\n",
      "        5.02912998e+00,  3.03824854e+00, -2.17214775e+00, -1.00000000e+00,\n",
      "       -7.69946575e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 3.16712379e+00, -2.45537901e+00,  8.06134641e-02,  8.98765922e-01,\n",
      "        4.38130826e-01, -7.04387715e-03,  1.45492740e-02, -5.94636202e-01,\n",
      "        1.45733533e-02,  7.21471548e-01,  1.81999481e+00,  2.28969479e+00,\n",
      "        1.45378733e+00,  3.95569229e+00, -5.76167154e+00,  2.23629761e+00,\n",
      "        6.44852638e-01,  4.10454161e-02,  5.72122514e-01,  5.05129814e-01,\n",
      "       -9.01470959e-01, -6.30758345e-01,  7.56565630e-01,  2.87593865e+00,\n",
      "        5.02912998e+00,  3.03824854e+00, -2.17214775e+00, -1.00000000e+00,\n",
      "       -7.69946575e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.44498324,  0.35273734, -0.71961635,  0.13854057], dtype=float32), reward=[0.0], next_state=array([ 3.29001045, -2.2857945 ,  0.14828044,  0.88687581,  0.46089435,\n",
      "       -0.01476845,  0.02845343, -0.26398179,  0.03496853,  0.5953508 ,\n",
      "        1.40125179,  1.97865593,  0.50510424,  4.16574478, -5.34975767,\n",
      "        2.42781806,  0.610282  ,  0.07391374,  0.59275055,  0.52032632,\n",
      "       -1.26611698, -0.32590085,  0.49776536,  2.43041182,  6.31882477,\n",
      "        2.71513462, -1.88419724, -1.        , -7.77494669,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.29001045, -2.2857945 ,  0.14828044,  0.88687581,  0.46089435,\n",
      "       -0.01476845,  0.02845343, -0.26398179,  0.03496853,  0.5953508 ,\n",
      "        1.40125179,  1.97865593,  0.50510424,  4.16574478, -5.34975767,\n",
      "        2.42781806,  0.610282  ,  0.07391374,  0.59275055,  0.52032632,\n",
      "       -1.26611698, -0.32590085,  0.49776536,  2.43041182,  6.31882477,\n",
      "        2.71513462, -1.88419724, -1.        , -7.77494669,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.26810184,  0.2610472 , -0.5554334 ,  0.26046908], dtype=float32), reward=[0.0], next_state=array([ 3.39517593, -2.12586713,  0.17526715,  0.87549722,  0.48150608,\n",
      "       -0.01960471,  0.03566916, -0.17920104,  0.04554483,  0.58533776,\n",
      "        1.28730571,  2.00294209,  0.23826046,  4.34696007, -4.88882542,\n",
      "        2.57605052,  0.57468456,  0.11011794,  0.60270441,  0.54254884,\n",
      "       -1.36508262, -0.23405685,  0.37909877,  2.14943075,  7.15706682,\n",
      "        2.3208425 , -1.59364128, -1.        , -7.83966255,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.39517593, -2.12586713,  0.17526715,  0.87549722,  0.48150608,\n",
      "       -0.01960471,  0.03566916, -0.17920104,  0.04554483,  0.58533776,\n",
      "        1.28730571,  2.00294209,  0.23826046,  4.34696007, -4.88882542,\n",
      "        2.57605052,  0.57468456,  0.11011794,  0.60270441,  0.54254884,\n",
      "       -1.36508262, -0.23405685,  0.37909877,  2.14943075,  7.15706682,\n",
      "        2.3208425 , -1.59364128, -1.        , -7.83966255,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.1834656 ,  0.3913642 , -0.56812143,  0.00996544], dtype=float32), reward=[0.0], next_state=array([ 3.48437691e+00, -1.97692895e+00,  1.82908848e-01,  8.64777327e-01,\n",
      "        5.00077188e-01, -2.28427518e-02,  3.95106636e-02, -7.73516968e-02,\n",
      "        4.53620441e-02,  5.05315185e-01,  1.03517640e+00,  1.76451540e+00,\n",
      "        6.05757050e-05,  4.50329018e+00, -4.39273882e+00,  2.69637442e+00,\n",
      "        5.39966226e-01,  1.46887705e-01,  6.02167428e-01,  5.69433868e-01,\n",
      "       -1.45845783e+00, -3.63988467e-02,  1.18838981e-01,  1.85542703e+00,\n",
      "        7.88997841e+00,  2.05928373e+00, -1.30087662e+00, -1.00000000e+00,\n",
      "       -7.89352369e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 3.48437691e+00, -1.97692895e+00,  1.82908848e-01,  8.64777327e-01,\n",
      "        5.00077188e-01, -2.28427518e-02,  3.95106636e-02, -7.73516968e-02,\n",
      "        4.53620441e-02,  5.05315185e-01,  1.03517640e+00,  1.76451540e+00,\n",
      "        6.05757050e-05,  4.50329018e+00, -4.39273882e+00,  2.69637442e+00,\n",
      "        5.39966226e-01,  1.46887705e-01,  6.02167428e-01,  5.69433868e-01,\n",
      "       -1.45845783e+00, -3.63988467e-02,  1.18838981e-01,  1.85542703e+00,\n",
      "        7.88997841e+00,  2.05928373e+00, -1.30087662e+00, -1.00000000e+00,\n",
      "       -7.89352369e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.24213102,  0.24134368, -0.5715529 ,  0.08690635], dtype=float32), reward=[0.0], next_state=array([ 3.5510006 , -1.85637832,  0.17586525,  0.8560341 ,  0.51480472,\n",
      "       -0.02407282,  0.04002884,  0.019194  ,  0.03586399,  0.38240179,\n",
      "        0.73418838,  1.34897232, -0.1633662 ,  4.63227654, -3.87915421,\n",
      "        2.78957963,  0.50770104,  0.18294646,  0.5881353 ,  0.60238457,\n",
      "       -1.53986216,  0.14563951, -0.20430414,  1.59953308,  8.44982052,\n",
      "        1.66626322, -1.00631142, -1.        , -7.9364562 ,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.5510006 , -1.85637832,  0.17586525,  0.8560341 ,  0.51480472,\n",
      "       -0.02407282,  0.04002884,  0.019194  ,  0.03586399,  0.38240179,\n",
      "        0.73418838,  1.34897232, -0.1633662 ,  4.63227654, -3.87915421,\n",
      "        2.78957963,  0.50770104,  0.18294646,  0.5881353 ,  0.60238457,\n",
      "       -1.53986216,  0.14563951, -0.20430414,  1.59953308,  8.44982052,\n",
      "        1.66626322, -1.00631142, -1.        , -7.9364562 ,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.5202354 ,  0.36792976, -0.8731938 , -0.03573056], dtype=float32), reward=[0.0], next_state=array([ 3.57659149e+00, -1.80885780e+00,  1.54000700e-01,  8.52636874e-01,\n",
      "        5.20813763e-01, -2.18910202e-02,  3.58370245e-02,  1.81367591e-01,\n",
      "        4.59513627e-03,  5.27131483e-02,  9.72859189e-02,  1.59491554e-01,\n",
      "       -3.48630250e-01,  4.72162056e+00, -3.36294961e+00,  2.85064077e+00,\n",
      "        4.76968944e-01,  2.18065575e-01,  5.51362216e-01,  6.48804843e-01,\n",
      "       -1.87026119e+00,  4.04268295e-01, -8.97759020e-01,  1.28845620e+00,\n",
      "        9.43568993e+00,  1.18448830e+00, -7.10353851e-01, -1.00000000e+00,\n",
      "       -7.96840000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 3.57659149e+00, -1.80885780e+00,  1.54000700e-01,  8.52636874e-01,\n",
      "        5.20813763e-01, -2.18910202e-02,  3.58370245e-02,  1.81367591e-01,\n",
      "        4.59513627e-03,  5.27131483e-02,  9.72859189e-02,  1.59491554e-01,\n",
      "       -3.48630250e-01,  4.72162056e+00, -3.36294961e+00,  2.85064077e+00,\n",
      "        4.76968944e-01,  2.18065575e-01,  5.51362216e-01,  6.48804843e-01,\n",
      "       -1.87026119e+00,  4.04268295e-01, -8.97759020e-01,  1.28845620e+00,\n",
      "        9.43568993e+00,  1.18448830e+00, -7.10353851e-01, -1.00000000e+00,\n",
      "       -7.96840000e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.48709455,  0.53629017, -0.90493786, -0.13005623], dtype=float32), reward=[0.0], next_state=array([ 3.55059433, -1.85938835,  0.1489062 ,  0.85635   ,  0.51488078,\n",
      "       -0.0203689 ,  0.03387515,  0.02627598, -0.02313074, -0.29498205,\n",
      "       -0.55143499, -1.05453873,  0.03357073,  4.78182602, -2.84937906,\n",
      "        2.87919569,  0.4401288 ,  0.2516363 ,  0.48756045,  0.71080995,\n",
      "       -2.31842399,  0.47885308, -1.50551236,  1.45103168, 10.41780567,\n",
      "        0.24047044, -0.41341209, -1.        , -7.98931122,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.55059433, -1.85938835,  0.1489062 ,  0.85635   ,  0.51488078,\n",
      "       -0.0203689 ,  0.03387515,  0.02627598, -0.02313074, -0.29498205,\n",
      "       -0.55143499, -1.05453873,  0.03357073,  4.78182602, -2.84937906,\n",
      "        2.87919569,  0.4401288 ,  0.2516363 ,  0.48756045,  0.71080995,\n",
      "       -2.31842399,  0.47885308, -1.50551236,  1.45103168, 10.41780567,\n",
      "        0.24047044, -0.41341209, -1.        , -7.98931122,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.6645161 ,  0.26515034, -1.        , -0.03739711], dtype=float32), reward=[0.0], next_state=array([ 3.4712429 , -1.99613535,  0.22172537,  0.86601013,  0.49700671,\n",
      "       -0.02734611,  0.04757205, -0.52891552, -0.05976711, -0.61202961,\n",
      "       -1.22080719, -2.03773999,  1.2540139 ,  4.83376217, -2.34147382,\n",
      "        2.88007522,  0.38542327,  0.28142208,  0.39873984,  0.78310728,\n",
      "       -2.90028024,  0.34321803, -1.95535791,  1.95596457, 11.11987305,\n",
      "       -1.29695261, -0.11589622, -1.        , -7.99916029,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.4712429 , -1.99613535,  0.22172537,  0.86601013,  0.49700671,\n",
      "       -0.02734611,  0.04757205, -0.52891552, -0.05976711, -0.61202961,\n",
      "       -1.22080719, -2.03773999,  1.2540139 ,  4.83376217, -2.34147382,\n",
      "        2.88007522,  0.38542327,  0.28142208,  0.39873984,  0.78310728,\n",
      "       -2.90028024,  0.34321803, -1.95535791,  1.95596457, 11.11987305,\n",
      "       -1.29695261, -0.11589622, -1.        , -7.99916029,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.7071366 ,  0.02657336, -1.        ,  0.22974725], dtype=float32), reward=[0.0], next_state=array([ 3.37866783, -2.12046289,  0.40993008,  0.87410849,  0.47641534,\n",
      "       -0.04536463,  0.08309594, -1.1248883 , -0.09048861, -0.54179066,\n",
      "       -1.17683983, -1.44706571,  2.68508935,  4.90824032, -1.83891487,\n",
      "        2.84993052,  0.30904937,  0.30281198,  0.30422017,  0.84867162,\n",
      "       -3.10356045,  0.0203281 , -1.74744844,  2.43395519, 10.54040241,\n",
      "       -3.13199234,  0.18177605, -1.        , -7.99793434,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.37866783, -2.12046289,  0.40993008,  0.87410849,  0.47641534,\n",
      "       -0.04536463,  0.08309594, -1.1248883 , -0.09048861, -0.54179066,\n",
      "       -1.17683983, -1.44706571,  2.68508935,  4.90824032, -1.83891487,\n",
      "        2.84993052,  0.30904937,  0.30281198,  0.30422017,  0.84867162,\n",
      "       -3.10356045,  0.0203281 , -1.74744844,  2.43395519, 10.54040241,\n",
      "       -3.13199234,  0.18177605, -1.        , -7.99793434,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.57748795,  0.0075242 , -0.93777204,  0.04995315], dtype=float32), reward=[0.0], next_state=array([ 3.26376343, -2.21346641,  0.72787881,  0.87886512,  0.45042601,\n",
      "       -0.07180858,  0.13984285, -1.7157129 , -0.19107708, -0.66504133,\n",
      "       -1.59463668, -1.1018647 ,  4.43052101,  5.00286865, -1.36172271,\n",
      "        2.77592468,  0.20745499,  0.31094518,  0.19844256,  0.90603322,\n",
      "       -3.66554236, -0.34000662, -1.93323696,  3.15883279,  9.48965359,\n",
      "       -5.54033852,  0.47920036, -1.        , -7.9856348 ,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.26376343, -2.21346641,  0.72787881,  0.87886512,  0.45042601,\n",
      "       -0.07180858,  0.13984285, -1.7157129 , -0.19107708, -0.66504133,\n",
      "       -1.59463668, -1.1018647 ,  4.43052101,  5.00286865, -1.36172271,\n",
      "        2.77592468,  0.20745499,  0.31094518,  0.19844256,  0.90603322,\n",
      "       -3.66554236, -0.34000662, -1.93323696,  3.15883279,  9.48965359,\n",
      "       -5.54033852,  0.47920036, -1.        , -7.9856348 ,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.3250708 ,  0.07357854, -0.7855026 , -0.13773973], dtype=float32), reward=[0.0], next_state=array([ 3.11397362, -2.22224021,  1.20661831,  0.87600422,  0.41555142,\n",
      "       -0.10511186,  0.22109993, -2.42366242, -0.35805601, -0.75134206,\n",
      "       -2.07706189,  0.22708197,  6.59192371,  5.12450504, -0.94421768,\n",
      "        2.67178941,  0.07451084,  0.29744676,  0.07750425,  0.9486658 ,\n",
      "       -4.37903404, -0.84373027, -2.05781627,  4.09621954,  7.04194498,\n",
      "       -7.65069437,  0.77596092, -1.        , -7.96227884,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.11397362, -2.22224021,  1.20661831,  0.87600422,  0.41555142,\n",
      "       -0.10511186,  0.22109993, -2.42366242, -0.35805601, -0.75134206,\n",
      "       -2.07706189,  0.22708197,  6.59192371,  5.12450504, -0.94421768,\n",
      "        2.67178941,  0.07451084,  0.29744676,  0.07750425,  0.9486658 ,\n",
      "       -4.37903404, -0.84373027, -2.05781627,  4.09621954,  7.04194498,\n",
      "       -7.65069437,  0.77596092, -1.        , -7.96227884,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.6559793 ,  0.0780295 , -0.40287852, -0.03301848], dtype=float32), reward=[0.0], next_state=array([ 2.98662567, -2.05196095,  1.72236836,  0.8591733 ,  0.38230541,\n",
      "       -0.13837774,  0.31066927, -2.4118259 , -0.3166872 , -0.42125025,\n",
      "       -1.40464318,  2.59050941,  6.09465313,  5.30448341, -0.62184274,\n",
      "        2.50677586, -0.07607736,  0.25671232, -0.03636309,  0.96280253,\n",
      "       -4.36746073, -1.26926959, -1.53812528,  4.85134268,  3.0024519 ,\n",
      "       -8.84601307,  1.07164574, -1.        , -7.92789888,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 2.98662567, -2.05196095,  1.72236836,  0.8591733 ,  0.38230541,\n",
      "       -0.13837774,  0.31066927, -2.4118259 , -0.3166872 , -0.42125025,\n",
      "       -1.40464318,  2.59050941,  6.09465313,  5.30448341, -0.62184274,\n",
      "        2.50677586, -0.07607736,  0.25671232, -0.03636309,  0.96280253,\n",
      "       -4.36746073, -1.26926959, -1.53812528,  4.85134268,  3.0024519 ,\n",
      "       -8.84601307,  1.07164574, -1.        , -7.92789888,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.11139446, -0.3137617 , -0.27266744,  0.2796837 ], dtype=float32), reward=[0.0], next_state=array([ 2.90853119, -1.68992221,  2.191221  ,  0.82612026,  0.35356897,\n",
      "       -0.17271672,  0.40334004, -2.74565029, -0.24100845, -0.2105898 ,\n",
      "       -0.88439316,  5.09574938,  5.69883442,  5.5097599 , -0.3901341 ,\n",
      "        2.3080647 , -0.24114338,  0.1855762 , -0.12423773,  0.94444495,\n",
      "       -4.83039665, -1.75038016, -0.66061389,  4.41137934, -0.55671656,\n",
      "       -8.55838013,  1.36584663, -1.        , -7.88254166,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 2.90853119, -1.68992221,  2.191221  ,  0.82612026,  0.35356897,\n",
      "       -0.17271672,  0.40334004, -2.74565029, -0.24100845, -0.2105898 ,\n",
      "       -0.88439316,  5.09574938,  5.69883442,  5.5097599 , -0.3901341 ,\n",
      "        2.3080647 , -0.24114338,  0.1855762 , -0.12423773,  0.94444495,\n",
      "       -4.83039665, -1.75038016, -0.66061389,  4.41137934, -0.55671656,\n",
      "       -8.55838013,  1.36584663, -1.        , -7.88254166,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.09475885, -0.21611938, -0.3294331 , -0.01897363], dtype=float32), reward=[0.0], next_state=array([ 2.88257027, -1.13425088,  2.55909443,  0.77375501,  0.32694069,\n",
      "       -0.21119206,  0.49981076, -3.05231309, -0.05365896, -0.02830832,\n",
      "       -0.1702769 ,  7.49518585,  4.1526227 ,  5.7380209 , -0.23722154,\n",
      "        2.11253786, -0.42366552,  0.07904188, -0.1717204 ,  0.88587356,\n",
      "       -5.49536228, -2.04212451,  0.58428568,  4.53841686, -4.02886677,\n",
      "       -7.06492233,  1.65815735, -1.        , -7.82627058,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 2.88257027, -1.13425088,  2.55909443,  0.77375501,  0.32694069,\n",
      "       -0.21119206,  0.49981076, -3.05231309, -0.05365896, -0.02830832,\n",
      "       -0.1702769 ,  7.49518585,  4.1526227 ,  5.7380209 , -0.23722154,\n",
      "        2.11253786, -0.42366552,  0.07904188, -0.1717204 ,  0.88587356,\n",
      "       -5.49536228, -2.04212451,  0.58428568,  4.53841686, -4.02886677,\n",
      "       -7.06492233,  1.65815735, -1.        , -7.82627058,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.19859208, -0.22747612, -0.2536557 , -0.15012996], dtype=float32), reward=[0.0], next_state=array([ 2.93143845, -0.45449686,  2.71533346,  0.70266616,  0.30428225,\n",
      "       -0.25544888,  0.59026986, -3.11646986,  0.29885021,  0.07196696,\n",
      "        0.85380697,  8.62595749,  1.15322459,  6.02515125, -0.16667236,\n",
      "        1.93003857, -0.60807151, -0.05580938, -0.1532073 ,  0.77695686,\n",
      "       -6.05881834, -1.66500199,  2.35692644,  4.96616411, -6.4788723 ,\n",
      "       -4.26001453,  1.94817162, -1.        , -7.75916386,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 2.93143845, -0.45449686,  2.71533346,  0.70266616,  0.30428225,\n",
      "       -0.25544888,  0.59026986, -3.11646986,  0.29885021,  0.07196696,\n",
      "        0.85380697,  8.62595749,  1.15322459,  6.02515125, -0.16667236,\n",
      "        1.93003857, -0.60807151, -0.05580938, -0.1532073 ,  0.77695686,\n",
      "       -6.05881834, -1.66500199,  2.35692644,  4.96616411, -6.4788723 ,\n",
      "       -4.26001453,  1.94817162, -1.        , -7.75916386,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.24777535, -0.11849995, -0.18005869, -0.16252662], dtype=float32), reward=[0.0], next_state=array([ 3.05990601e+00,  2.08019257e-01,  2.60586882e+00,  6.18757546e-01,\n",
      "        2.85813242e-01, -3.06557149e-01,  6.64434075e-01, -2.98476076e+00,\n",
      "        6.85855091e-01, -7.54971849e-03,  1.82264781e+00,  7.90809584e+00,\n",
      "       -2.16688681e+00,  6.38289261e+00, -1.75558597e-01,  1.76366830e+00,\n",
      "       -7.64445245e-01, -2.05056593e-01, -3.20016779e-02,  6.10369682e-01,\n",
      "       -6.62322521e+00, -1.64873376e-01,  4.41801929e+00,  5.43421268e+00,\n",
      "       -7.30864525e+00, -1.19013822e+00,  2.23548889e+00, -1.00000000e+00,\n",
      "       -7.68131399e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 3.05990601e+00,  2.08019257e-01,  2.60586882e+00,  6.18757546e-01,\n",
      "        2.85813242e-01, -3.06557149e-01,  6.64434075e-01, -2.98476076e+00,\n",
      "        6.85855091e-01, -7.54971849e-03,  1.82264781e+00,  7.90809584e+00,\n",
      "       -2.16688681e+00,  6.38289261e+00, -1.75558597e-01,  1.76366830e+00,\n",
      "       -7.64445245e-01, -2.05056593e-01, -3.20016779e-02,  6.10369682e-01,\n",
      "       -6.62322521e+00, -1.64873376e-01,  4.41801929e+00,  5.43421268e+00,\n",
      "       -7.30864525e+00, -1.19013822e+00,  2.23548889e+00, -1.00000000e+00,\n",
      "       -7.68131399e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.31637585, -0.01934345, -0.15002659, -0.24093653], dtype=float32), reward=[0.0], next_state=array([ 3.24056625,  0.75538737,  2.26827049,  0.52512664,  0.26610342,\n",
      "       -0.36494946,  0.72127867, -2.95004439,  0.91807687, -0.2370889 ,\n",
      "        2.32106447,  6.23211288, -4.74794769,  6.7829113 , -0.22555614,\n",
      "        1.61673355, -0.84682792, -0.31079376,  0.1851809 ,  0.38986883,\n",
      "       -6.89468908,  2.23213243,  4.94435358,  5.37904501, -6.29220867,\n",
      "        1.31301343,  2.51971245, -1.        , -7.59282875,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.24056625,  0.75538737,  2.26827049,  0.52512664,  0.26610342,\n",
      "       -0.36494946,  0.72127867, -2.95004439,  0.91807687, -0.2370889 ,\n",
      "        2.32106447,  6.23211288, -4.74794769,  6.7829113 , -0.22555614,\n",
      "        1.61673355, -0.84682792, -0.31079376,  0.1851809 ,  0.38986883,\n",
      "       -6.89468908,  2.23213243,  4.94435358,  5.37904501, -6.29220867,\n",
      "        1.31301343,  2.51971245, -1.        , -7.59282875,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.5573103 ,  0.38897204,  0.1048199 , -0.34359303], dtype=float32), reward=[0.0], next_state=array([ 3.42785645,  1.14282846,  1.78197312,  0.42231175,  0.23676406,\n",
      "       -0.42727855,  0.76356316, -3.15036035,  0.92612153, -0.50174308,\n",
      "        2.29519749,  4.332129  , -6.41488028,  7.14534187, -0.26653099,\n",
      "        1.47073507, -0.83379781, -0.31812021,  0.41963258,  0.16579869,\n",
      "       -6.04843855,  2.82952213,  2.53022766,  4.15795183, -4.50135803,\n",
      "        2.66048384,  2.80044365, -1.        , -7.49383163,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.42785645,  1.14282846,  1.78197312,  0.42231175,  0.23676406,\n",
      "       -0.42727855,  0.76356316, -3.15036035,  0.92612153, -0.50174308,\n",
      "        2.29519749,  4.332129  , -6.41488028,  7.14534187, -0.26653099,\n",
      "        1.47073507, -0.83379781, -0.31812021,  0.41963258,  0.16579869,\n",
      "       -6.04843855,  2.82952213,  2.53022766,  4.15795183, -4.50135803,\n",
      "        2.66048384,  2.80044365, -1.        , -7.49383163,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.53533816,  0.14884803,  0.08933159, -0.30121037], dtype=float32), reward=[0.0], next_state=array([ 3.59802437e+00,  1.32734954e+00,  1.23555100e+00,  3.15265596e-01,\n",
      "        1.95400417e-01, -4.88446742e-01,  7.89839327e-01, -3.07090640e+00,\n",
      "        7.94296443e-01, -7.37430573e-01,  2.03523445e+00,  1.59219038e+00,\n",
      "       -6.76042461e+00,  7.42928028e+00, -3.60430002e-01,  1.34129715e+00,\n",
      "       -7.85748124e-01, -2.42383465e-01,  5.69068730e-01, -3.32926912e-03,\n",
      "       -5.02736568e+00,  2.29094720e+00,  7.48115301e-01,  2.93265438e+00,\n",
      "       -3.58055401e+00,  3.52383900e+00,  3.07730103e+00, -1.00000000e+00,\n",
      "       -7.38445807e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 3.59802437e+00,  1.32734954e+00,  1.23555100e+00,  3.15265596e-01,\n",
      "        1.95400417e-01, -4.88446742e-01,  7.89839327e-01, -3.07090640e+00,\n",
      "        7.94296443e-01, -7.37430573e-01,  2.03523445e+00,  1.59219038e+00,\n",
      "       -6.76042461e+00,  7.42928028e+00, -3.60430002e-01,  1.34129715e+00,\n",
      "       -7.85748124e-01, -2.42383465e-01,  5.69068730e-01, -3.32926912e-03,\n",
      "       -5.02736568e+00,  2.29094720e+00,  7.48115301e-01,  2.93265438e+00,\n",
      "       -3.58055401e+00,  3.52383900e+00,  3.07730103e+00, -1.00000000e+00,\n",
      "       -7.38445807e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.525487  , -0.258843  ,  0.12477035, -0.35233164], dtype=float32), reward=[0.0], next_state=array([ 3.74260521,  1.30481577,  0.72065175,  0.21013209,  0.14361505,\n",
      "       -0.54456174,  0.79916942, -2.99421239,  0.59926176, -0.92209125,\n",
      "        1.71133161, -0.89367723, -6.1378274 ,  7.62707138, -0.54108763,\n",
      "        1.25264263, -0.72786838, -0.13838984,  0.66001654, -0.12423407,\n",
      "       -4.16871071,  1.64133048, -0.20037714,  1.66415811, -3.14168978,\n",
      "        3.82270241,  3.34989738, -1.        , -7.26486015,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.74260521,  1.30481577,  0.72065175,  0.21013209,  0.14361505,\n",
      "       -0.54456174,  0.79916942, -2.99421239,  0.59926176, -0.92209125,\n",
      "        1.71133161, -0.89367723, -6.1378274 ,  7.62707138, -0.54108763,\n",
      "        1.25264263, -0.72786838, -0.13838984,  0.66001654, -0.12423407,\n",
      "       -4.16871071,  1.64133048, -0.20037714,  1.66415811, -3.14168978,\n",
      "        3.82270241,  3.34989738, -1.        , -7.26486015,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.23741655, -0.43217555,  0.04944853, -0.34994152], dtype=float32), reward=[0.0], next_state=array([ 3.86730766,  1.10121477,  0.23466928,  0.0877711 ,  0.06690416,\n",
      "       -0.59871387,  0.79332328, -4.0011344 ,  0.37668628, -1.19627059,\n",
      "        1.50833106, -3.17391753, -6.04429054,  7.76144505, -0.82129276,\n",
      "        1.17817092, -0.65274692, -0.01859114,  0.72584504, -0.21615945,\n",
      "       -4.15250683,  1.84039509, -0.79419285,  0.94869405, -3.19777179,\n",
      "        3.66718698,  3.61785316, -1.        , -7.13520432,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.86730766,  1.10121477,  0.23466928,  0.0877711 ,  0.06690416,\n",
      "       -0.59871387,  0.79332328, -4.0011344 ,  0.37668628, -1.19627059,\n",
      "        1.50833106, -3.17391753, -6.04429054,  7.76144505, -0.82129276,\n",
      "        1.17817092, -0.65274692, -0.01859114,  0.72584504, -0.21615945,\n",
      "       -4.15250683,  1.84039509, -0.79419285,  0.94869405, -3.19777179,\n",
      "        3.66718698,  3.61785316, -1.        , -7.13520432,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.01334868, -0.46897364, -0.31010145, -0.15711051], dtype=float32), reward=[0.0], next_state=array([ 3.9609127 ,  0.68851238, -0.21770564, -0.11248408, -0.09329647,\n",
      "       -0.63636833,  0.75741571, -7.25491142, -0.17196169, -1.23643148,\n",
      "        0.99390489, -5.78074646, -5.02787685,  7.86665535, -1.23805106,\n",
      "        1.12352204, -0.45965415,  0.1468468 ,  0.8312636 , -0.27596182,\n",
      "       -5.56148243,  5.82417011, -3.2915554 ,  1.02869785, -3.72531796,\n",
      "        3.63609648,  3.88080215, -1.        , -6.99566841,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.9609127 ,  0.68851238, -0.21770564, -0.11248408, -0.09329647,\n",
      "       -0.63636833,  0.75741571, -7.25491142, -0.17196169, -1.23643148,\n",
      "        0.99390489, -5.78074646, -5.02787685,  7.86665535, -1.23805106,\n",
      "        1.12352204, -0.45965415,  0.1468468 ,  0.8312636 , -0.27596182,\n",
      "       -5.56148243,  5.82417011, -3.2915554 ,  1.02869785, -3.72531796,\n",
      "        3.63609648,  3.88080215, -1.        , -6.99566841,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.2983762 , -0.16937672, -0.05015639, -0.07860851], dtype=float32), reward=[0.0], next_state=array([ 3.9962101 ,  0.38242817, -0.25201401, -0.2207962 , -0.19755816,\n",
      "       -0.63727915,  0.71140361, -2.40128112, -0.40631235, -0.65090406,\n",
      "        0.3852976 , -3.24104905,  0.60173291,  7.90173817, -1.58523202,\n",
      "        1.33195341, -0.10674231,  0.29512995,  0.92798537, -0.20086691,\n",
      "       -3.32765484,  5.66082525, -4.52408743, -0.01252005, -3.37129569,\n",
      "        4.42368793,  4.13837719, -1.        , -6.84644699,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.9962101 ,  0.38242817, -0.25201401, -0.2207962 , -0.19755816,\n",
      "       -0.63727915,  0.71140361, -2.40128112, -0.40631235, -0.65090406,\n",
      "        0.3852976 , -3.24104905,  0.60173291,  7.90173817, -1.58523202,\n",
      "        1.33195341, -0.10674231,  0.29512995,  0.92798537, -0.20086691,\n",
      "       -3.32765484,  5.66082525, -4.52408743, -0.01252005, -3.37129569,\n",
      "        4.42368793,  4.13837719, -1.        , -6.84644699,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.46601978, -0.477838  , -0.11348709, -0.02581464], dtype=float32), reward=[0.0], next_state=array([ 4.01625824,  0.14821476, -0.13469622, -0.27900511, -0.26613504,\n",
      "       -0.63724679,  0.66726667, -2.51819372, -0.55284178, -0.605497  ,\n",
      "        0.2064753 , -2.86296821,  1.75529146,  7.91763306, -1.89983606,\n",
      "        1.65899229,  0.17792477,  0.3658894 ,  0.90855461, -0.09484849,\n",
      "       -2.86124682,  5.38791943, -4.73838377,  0.0956061 , -3.44254994,\n",
      "        4.99046659,  4.39022064, -1.        , -6.687747  ,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 4.01625824,  0.14821476, -0.13469622, -0.27900511, -0.26613504,\n",
      "       -0.63724679,  0.66726667, -2.51819372, -0.55284178, -0.605497  ,\n",
      "        0.2064753 , -2.86296821,  1.75529146,  7.91763306, -1.89983606,\n",
      "        1.65899229,  0.17792477,  0.3658894 ,  0.90855461, -0.09484849,\n",
      "       -2.86124682,  5.38791943, -4.73838377,  0.0956061 , -3.44254994,\n",
      "        4.99046659,  4.39022064, -1.        , -6.687747  ,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.60904515, -0.49300772, -0.09170898, -0.14251024], dtype=float32), reward=[0.0], next_state=array([ 4.01963329e+00, -2.76813358e-02,  7.67622292e-02, -3.63566875e-01,\n",
      "       -3.71251523e-01, -6.11017704e-01,  5.97200871e-01, -4.49805164e+00,\n",
      "       -7.35448956e-01, -4.77483481e-01, -1.61977094e-02, -1.86360824e+00,\n",
      "        3.02302980e+00,  7.92450523e+00, -2.18070698e+00,  2.05655599e+00,\n",
      "        4.40953314e-01,  3.82820666e-01,  8.11489224e-01,  2.22211853e-02,\n",
      "       -2.25028014e+00,  5.32626677e+00, -4.84648132e+00,  2.22861278e-03,\n",
      "       -3.31779480e+00,  5.18106365e+00,  4.63598824e+00, -1.00000000e+00,\n",
      "       -6.51978636e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 4.01963329e+00, -2.76813358e-02,  7.67622292e-02, -3.63566875e-01,\n",
      "       -3.71251523e-01, -6.11017704e-01,  5.97200871e-01, -4.49805164e+00,\n",
      "       -7.35448956e-01, -4.77483481e-01, -1.61977094e-02, -1.86360824e+00,\n",
      "        3.02302980e+00,  7.92450523e+00, -2.18070698e+00,  2.05655599e+00,\n",
      "        4.40953314e-01,  3.82820666e-01,  8.11489224e-01,  2.22211853e-02,\n",
      "       -2.25028014e+00,  5.32626677e+00, -4.84648132e+00,  2.22861278e-03,\n",
      "       -3.31779480e+00,  5.18106365e+00,  4.63598824e+00, -1.00000000e+00,\n",
      "       -6.51978636e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.44799703, -0.30178052,  0.12833765, -0.49958968], dtype=float32), reward=[0.0], next_state=array([ 3.99803162e+00,  6.74953312e-03,  4.31924969e-01, -4.70427781e-01,\n",
      "       -5.23146152e-01, -5.28953254e-01,  4.74578023e-01, -5.46233559e+00,\n",
      "       -1.12158513e+00, -1.17770635e-01, -3.80548477e-01,  1.35517943e+00,\n",
      "        4.74426126e+00,  7.84988403e+00, -2.28064156e+00,  2.50624180e+00,\n",
      "        5.79476714e-01,  3.39945823e-01,  7.37249136e-01,  7.14652911e-02,\n",
      "        4.70509827e-01,  2.03324914e+00, -2.06085277e+00, -1.98657024e+00,\n",
      "       -1.61951530e+00,  4.03265476e+00,  4.87533569e+00, -1.00000000e+00,\n",
      "       -6.34279871e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 3.99803162e+00,  6.74953312e-03,  4.31924969e-01, -4.70427781e-01,\n",
      "       -5.23146152e-01, -5.28953254e-01,  4.74578023e-01, -5.46233559e+00,\n",
      "       -1.12158513e+00, -1.17770635e-01, -3.80548477e-01,  1.35517943e+00,\n",
      "        4.74426126e+00,  7.84988403e+00, -2.28064156e+00,  2.50624180e+00,\n",
      "        5.79476714e-01,  3.39945823e-01,  7.37249136e-01,  7.14652911e-02,\n",
      "        4.70509827e-01,  2.03324914e+00, -2.06085277e+00, -1.98657024e+00,\n",
      "       -1.61951530e+00,  4.03265476e+00,  4.87533569e+00, -1.00000000e+00,\n",
      "       -6.34279871e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.80083984, -0.11823696,  0.1078614 , -0.3996069 ], dtype=float32), reward=[0.0], next_state=array([ 3.94019699,  0.16443409,  0.78938484, -0.48677447, -0.59565461,\n",
      "       -0.49489978,  0.40412924, -1.93984997, -1.09243572,  0.16711877,\n",
      "       -0.7894699 ,  2.03224158,  4.12066221,  7.63550854, -2.22781467,\n",
      "        2.85926723,  0.5977785 ,  0.28326783,  0.74768072,  0.05825658,\n",
      "        1.39373684, -0.03955086, -0.67577696, -3.9252553 , -0.45103243,\n",
      "        2.17280316,  5.107934  , -1.        , -6.15702915,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.94019699,  0.16443409,  0.78938484, -0.48677447, -0.59565461,\n",
      "       -0.49489978,  0.40412924, -1.93984997, -1.09243572,  0.16711877,\n",
      "       -0.7894699 ,  2.03224158,  4.12066221,  7.63550854, -2.22781467,\n",
      "        2.85926723,  0.5977785 ,  0.28326783,  0.74768072,  0.05825658,\n",
      "        1.39373684, -0.03955086, -0.67577696, -3.9252553 , -0.45103243,\n",
      "        2.17280316,  5.107934  , -1.        , -6.15702915,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.73078275, -0.03936473, -0.37131995, -0.35145667], dtype=float32), reward=[0.0], next_state=array([ 3.87910271,  0.29687819,  1.01938307, -0.48248765, -0.63136834,\n",
      "       -0.48241904,  0.36858055, -0.89366549, -0.60478061,  0.15420523,\n",
      "       -0.6288802 ,  1.47298014,  2.13236141,  7.34656048, -2.14089346,\n",
      "        3.07862329,  0.59322155,  0.23662946,  0.7672438 ,  0.05858155,\n",
      "        0.7022475 , -0.47790983, -0.78367674, -5.0865016 ,  0.4569076 ,\n",
      "        1.43894947,  5.33345795, -1.        , -5.96273613,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.87910271,  0.29687819,  1.01938307, -0.48248765, -0.63136834,\n",
      "       -0.48241904,  0.36858055, -0.89366549, -0.60478061,  0.15420523,\n",
      "       -0.6288802 ,  1.47298014,  2.13236141,  7.34656048, -2.14089346,\n",
      "        3.07862329,  0.59322155,  0.23662946,  0.7672438 ,  0.05858155,\n",
      "        0.7022475 , -0.47790983, -0.78367674, -5.0865016 ,  0.4569076 ,\n",
      "        1.43894947,  5.33345795, -1.        , -5.96273613,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.6151667 ,  0.08011021,  0.04995109, -0.36397776], dtype=float32), reward=[0.0], next_state=array([ 3.83266258,  0.40476677,  1.15088665, -0.47953838, -0.6547882 ,\n",
      "       -0.47134578,  0.34514996, -0.68970013, -0.41553921,  0.1331522 ,\n",
      "       -0.51483732,  1.28678095,  1.34901714,  7.01356411, -2.04465437,\n",
      "        3.20009756,  0.58346993,  0.19533315,  0.78451014,  0.07714636,\n",
      "        0.53691936, -0.6340642 , -0.98504639, -6.01583052,  0.70871949,\n",
      "        0.55667901,  5.5515995 , -1.        , -5.76018572,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.83266258,  0.40476677,  1.15088665, -0.47953838, -0.6547882 ,\n",
      "       -0.47134578,  0.34514996, -0.68970013, -0.41553921,  0.1331522 ,\n",
      "       -0.51483732,  1.28678095,  1.34901714,  7.01356411, -2.04465437,\n",
      "        3.20009756,  0.58346993,  0.19533315,  0.78451014,  0.07714636,\n",
      "        0.53691936, -0.6340642 , -0.98504639, -6.01583052,  0.70871949,\n",
      "        0.55667901,  5.5515995 , -1.        , -5.76018572,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.24474731, -0.07463159,  0.05078613, -0.43761352], dtype=float32), reward=[0.0], next_state=array([ 3.79745865,  0.49023372,  1.23257935, -0.47734717, -0.67114043,\n",
      "       -0.46221757,  0.32873264, -0.47272158, -0.26778939,  0.09954351,\n",
      "       -0.37066421,  0.95368969,  0.80534673,  6.63673496, -1.95205104,\n",
      "        3.25219822,  0.56965029,  0.15240322,  0.80047125,  0.10732029,\n",
      "        0.39887401, -0.74633425, -1.22727704, -6.96025562,  0.83686626,\n",
      "       -0.14164679,  5.76205444, -1.        , -5.54966021,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.79745865,  0.49023372,  1.23257935, -0.47734717, -0.67114043,\n",
      "       -0.46221757,  0.32873264, -0.47272158, -0.26778939,  0.09954351,\n",
      "       -0.37066421,  0.95368969,  0.80534673,  6.63673496, -1.95205104,\n",
      "        3.25219822,  0.56965029,  0.15240322,  0.80047125,  0.10732029,\n",
      "        0.39887401, -0.74633425, -1.22727704, -6.96025562,  0.83686626,\n",
      "       -0.14164679,  5.76205444, -1.        , -5.54966021,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.36339152, -0.6363959 , -0.06424037, -0.34433582], dtype=float32), reward=[0.0], next_state=array([ 3.78610992e+00,  5.20604193e-01,  1.25505924e+00, -4.76926088e-01,\n",
      "       -6.76597297e-01, -4.58560169e-01,  3.23234051e-01, -7.20486790e-02,\n",
      "       -1.35883014e-03,  5.41026937e-04, -1.97603530e-03,  9.24901441e-02,\n",
      "       -3.08539607e-02,  6.23535728e+00, -1.89291716e+00,  3.24332762e+00,\n",
      "        5.56553483e-01,  1.10778965e-01,  8.08928668e-01,  1.53657913e-01,\n",
      "        5.65366335e-02, -5.90780914e-01, -1.59235585e+00, -7.54125452e+00,\n",
      "        7.22027838e-01, -5.29466927e-01,  5.96452999e+00, -1.00000000e+00,\n",
      "       -5.33145237e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 3.78610992e+00,  5.20604193e-01,  1.25505924e+00, -4.76926088e-01,\n",
      "       -6.76597297e-01, -4.58560169e-01,  3.23234051e-01, -7.20486790e-02,\n",
      "       -1.35883014e-03,  5.41026937e-04, -1.97603530e-03,  9.24901441e-02,\n",
      "       -3.08539607e-02,  6.23535728e+00, -1.89291716e+00,  3.24332762e+00,\n",
      "        5.56553483e-01,  1.10778965e-01,  8.08928668e-01,  1.53657913e-01,\n",
      "        5.65366335e-02, -5.90780914e-01, -1.59235585e+00, -7.54125452e+00,\n",
      "        7.22027838e-01, -5.29466927e-01,  5.96452999e+00, -1.00000000e+00,\n",
      "       -5.33145237e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.03357704, -0.8483136 , -0.35350528, -0.35394955], dtype=float32), reward=[0.0], next_state=array([ 3.81702805,  0.4440428 ,  1.18858767, -0.47881457, -0.66254187,\n",
      "       -0.46687341,  0.33734888,  0.74493104,  0.43849102, -0.16476475,\n",
      "        0.60775477, -1.53175139, -1.32870519,  5.83082294, -1.90461195,\n",
      "        3.18305516,  0.55579805,  0.07404041,  0.7965914 ,  0.22593941,\n",
      "       -0.47515059,  0.08185377, -2.28553057, -8.00072861,  0.506091  ,\n",
      "       -0.32232854,  6.15874863, -1.        , -5.10586119,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.81702805,  0.4440428 ,  1.18858767, -0.47881457, -0.66254187,\n",
      "       -0.46687341,  0.33734888,  0.74493104,  0.43849102, -0.16476475,\n",
      "        0.60775477, -1.53175139, -1.32870519,  5.83082294, -1.90461195,\n",
      "        3.18305516,  0.55579805,  0.07404041,  0.7965914 ,  0.22593941,\n",
      "       -0.47515059,  0.08185377, -2.28553057, -8.00072861,  0.506091  ,\n",
      "       -0.32232854,  6.15874863, -1.        , -5.10586119,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.11887152, -0.68377644, -0.5378802 , -0.23752515], dtype=float32), reward=[0.0], next_state=array([ 3.88081932,  0.23559727,  1.02943408, -0.47234088, -0.61701882,\n",
      "       -0.4999395 ,  0.38241655,  2.15972877,  0.77047038, -0.20176508,\n",
      "        0.88555843, -3.10375929, -2.37299895,  5.44237804, -1.96962464,\n",
      "        3.08635569,  0.58271402,  0.03473281,  0.7441678 ,  0.32473427,\n",
      "       -0.52263176,  1.2715354 , -3.1913085 , -7.77172565,  1.26717281,\n",
      "        0.04718782,  6.34443855, -1.        , -4.87320232,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.88081932,  0.23559727,  1.02943408, -0.47234088, -0.61701882,\n",
      "       -0.4999395 ,  0.38241655,  2.15972877,  0.77047038, -0.20176508,\n",
      "        0.88555843, -3.10375929, -2.37299895,  5.44237804, -1.96962464,\n",
      "        3.08635569,  0.58271402,  0.03473281,  0.7441678 ,  0.32473427,\n",
      "       -0.52263176,  1.2715354 , -3.1913085 , -7.77172565,  1.26717281,\n",
      "        0.04718782,  6.34443855, -1.        , -4.87320232,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.15704219, -0.5801002 , -0.17045721, -0.12356596], dtype=float32), reward=[0.0], next_state=array([ 3.93131447, -0.12347863,  0.83521581, -0.4068453 , -0.50269222,\n",
      "       -0.59319347,  0.47947794,  5.9001646 ,  0.57461399,  0.02276962,\n",
      "        0.50866711, -5.12425184, -2.49247289,  5.07436466, -2.10644364,\n",
      "        2.9827168 ,  0.67975277, -0.06102775,  0.59140456,  0.42947924,\n",
      "        1.93163943,  4.17430258, -5.70967293, -7.08296919,  1.12199807,\n",
      "        0.31079689,  6.52134228, -1.        , -4.63379908,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.93131447, -0.12347863,  0.83521581, -0.4068453 , -0.50269222,\n",
      "       -0.59319347,  0.47947794,  5.9001646 ,  0.57461399,  0.02276962,\n",
      "        0.50866711, -5.12425184, -2.49247289,  5.07436466, -2.10644364,\n",
      "        2.9827168 ,  0.67975277, -0.06102775,  0.59140456,  0.42947924,\n",
      "        1.93163943,  4.17430258, -5.70967293, -7.08296919,  1.12199807,\n",
      "        0.31079689,  6.52134228, -1.        , -4.63379908,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.02764738, -0.43183947,  0.10523923,  0.04307542], dtype=float32), reward=[0.0], next_state=array([ 3.92186356, -0.45226789,  0.74897313, -0.30099684, -0.37651363,\n",
      "       -0.68417346,  0.54730713,  3.66799569, -0.29737395, -0.16027361,\n",
      "       -0.28932431, -3.39792061, -0.31687981,  4.79485512, -2.25685811,\n",
      "        2.94722724,  0.81473535, -0.27112564,  0.27898893,  0.42995629,\n",
      "        4.82049942,  3.93663621, -5.93797874, -4.46016693,  0.02232912,\n",
      "        0.17915307,  6.68921852, -1.        , -4.38797903,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.92186356, -0.45226789,  0.74897313, -0.30099684, -0.37651363,\n",
      "       -0.68417346,  0.54730713,  3.66799569, -0.29737395, -0.16027361,\n",
      "       -0.28932431, -3.39792061, -0.31687981,  4.79485512, -2.25685811,\n",
      "        2.94722724,  0.81473535, -0.27112564,  0.27898893,  0.42995629,\n",
      "        4.82049942,  3.93663621, -5.93797874, -4.46016693,  0.02232912,\n",
      "        0.17915307,  6.68921852, -1.        , -4.38797903,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.13643995, -0.3007173 ,  0.28308862,  0.04742207], dtype=float32), reward=[0.0], next_state=array([ 3.88110352, -0.71109849,  0.75355953, -0.23616175, -0.30866867,\n",
      "       -0.73153776,  0.56018198,  2.48688006, -0.44448572, -0.39259243,\n",
      "       -0.59450501, -3.40061593,  0.0842148 ,  4.57179642, -2.44793439,\n",
      "        2.96622133,  0.83255851, -0.40903214,  0.07191887,  0.36656076,\n",
      "        3.58926964,  2.12923789, -4.15975714, -3.57342172, -1.31245959,\n",
      "        0.32491198,  6.84783173, -1.        , -4.13608503,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.88110352, -0.71109849,  0.75355953, -0.23616175, -0.30866867,\n",
      "       -0.73153776,  0.56018198,  2.48688006, -0.44448572, -0.39259243,\n",
      "       -0.59450501, -3.40061593,  0.0842148 ,  4.57179642, -2.44793439,\n",
      "        2.96622133,  0.83255851, -0.40903214,  0.07191887,  0.36656076,\n",
      "        3.58926964,  2.12923789, -4.15975714, -3.57342172, -1.31245959,\n",
      "        0.32491198,  6.84783173, -1.        , -4.13608503,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.04137854, -0.14791904,  0.07995912, -0.19294634], dtype=float32), reward=[0.0], next_state=array([ 3.80747986, -1.05558705,  0.71872061, -0.16811901, -0.23497197,\n",
      "       -0.77809411,  0.55775785,  2.75111842, -0.53000176, -0.71205187,\n",
      "       -1.08622909, -4.74498272, -0.66497284,  4.34982109, -2.74575949,\n",
      "        2.96918964,  0.79727876, -0.51941526, -0.11127506,  0.28665712,\n",
      "        3.32842803,  1.89306378, -4.24307871, -3.24819565, -2.2789917 ,\n",
      "        0.45457456,  6.99696541, -1.        , -3.87846303,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.80747986, -1.05558705,  0.71872061, -0.16811901, -0.23497197,\n",
      "       -0.77809411,  0.55775785,  2.75111842, -0.53000176, -0.71205187,\n",
      "       -1.08622909, -4.74498272, -0.66497284,  4.34982109, -2.74575949,\n",
      "        2.96918964,  0.79727876, -0.51941526, -0.11127506,  0.28665712,\n",
      "        3.32842803,  1.89306378, -4.24307871, -3.24819565, -2.2789917 ,\n",
      "        0.45457456,  6.99696541, -1.        , -3.87846303,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.01978416,  0.03971916, -0.3468095 , -0.22592539], dtype=float32), reward=[0.0], next_state=array([ 3.67831802, -1.46849537,  0.6542564 , -0.11147288, -0.17151864,\n",
      "       -0.82012546,  0.53436828,  1.9883554 , -0.52290452, -1.08248258,\n",
      "       -1.84594321, -5.35775757, -0.80259538,  4.09847832, -3.1075902 ,\n",
      "        2.96734715,  0.73078126, -0.6008442 , -0.23275332,  0.22532409,\n",
      "        1.76687479,  1.09138155, -3.39055634, -3.44709325, -2.37545753,\n",
      "        0.7270301 ,  7.13641071, -1.        , -3.61547279,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.67831802, -1.46849537,  0.6542564 , -0.11147288, -0.17151864,\n",
      "       -0.82012546,  0.53436828,  1.9883554 , -0.52290452, -1.08248258,\n",
      "       -1.84594321, -5.35775757, -0.80259538,  4.09847832, -3.1075902 ,\n",
      "        2.96734715,  0.73078126, -0.6008442 , -0.23275332,  0.22532409,\n",
      "        1.76687479,  1.09138155, -3.39055634, -3.44709325, -2.37545753,\n",
      "        0.7270301 ,  7.13641071, -1.        , -3.61547279,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.1600815 , -0.18110101, -0.10290228,  0.02348991], dtype=float32), reward=[0.0], next_state=array([ 3.47287178, -1.92953861,  0.56329477, -0.0692008 , -0.1210112 ,\n",
      "       -0.85879159,  0.49299547,  1.58874965, -0.45763433, -1.42913258,\n",
      "       -2.88570476, -5.98528433, -1.29140663,  3.79911613, -3.52525568,\n",
      "        2.94961739,  0.65119135, -0.66607589, -0.31583181,  0.18039684,\n",
      "        1.14292836,  0.95061857, -3.1070869 , -3.41930556, -3.16649008,\n",
      "        0.56861275,  7.26597595, -1.        , -3.34747529,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.47287178, -1.92953861,  0.56329477, -0.0692008 , -0.1210112 ,\n",
      "       -0.85879159,  0.49299547,  1.58874965, -0.45763433, -1.42913258,\n",
      "       -2.88570476, -5.98528433, -1.29140663,  3.79911613, -3.52525568,\n",
      "        2.94961739,  0.65119135, -0.66607589, -0.31583181,  0.18039684,\n",
      "        1.14292836,  0.95061857, -3.1070869 , -3.41930556, -3.16649008,\n",
      "        0.56861275,  7.26597595, -1.        , -3.34747529,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([-0.0217568 ,  0.08897128,  0.05133035, -0.3471105 ], dtype=float32), reward=[0.0], next_state=array([ 3.15291595, -2.44456458,  0.41654384, -0.03588619, -0.07481235,\n",
      "       -0.89718843,  0.43378359,  1.40721893, -0.36495748, -1.8695507 ,\n",
      "       -4.52695417, -6.71432352, -2.09674644,  3.44363976, -4.01063776,\n",
      "        2.89543772,  0.55115819, -0.72382402, -0.3891235 ,  0.14452089,\n",
      "        0.86585444,  1.25967681, -3.36217451, -3.12056208, -3.81588316,\n",
      "        0.53278208,  7.38548088, -1.        , -3.07484484,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 3.15291595, -2.44456458,  0.41654384, -0.03588619, -0.07481235,\n",
      "       -0.89718843,  0.43378359,  1.40721893, -0.36495748, -1.8695507 ,\n",
      "       -4.52695417, -6.71432352, -2.09674644,  3.44363976, -4.01063776,\n",
      "        2.89543772,  0.55115819, -0.72382402, -0.3891235 ,  0.14452089,\n",
      "        0.86585444,  1.25967681, -3.36217451, -3.12056208, -3.81588316,\n",
      "        0.53278208,  7.38548088, -1.        , -3.07484484,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.05477449,  0.170597  , -0.04249706, -0.06819011], dtype=float32), reward=[0.0], next_state=array([ 2.70978165, -2.94493175,  0.23987584, -0.01426059, -0.03736738,\n",
      "       -0.93176889,  0.36084214,  0.99856395, -0.20397425, -2.05498862,\n",
      "       -5.90556669, -6.1014204 , -2.26402855,  3.03112411, -4.52418995,\n",
      "        2.81378031,  0.43737584, -0.76855147, -0.45171213,  0.11826754,\n",
      "        0.63191301,  1.38548589, -3.12825918, -2.94066858, -4.47891855,\n",
      "        0.26958263,  7.49476051, -1.        , -2.79795647,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), done=[False]), Experience(state=array([ 2.70978165, -2.94493175,  0.23987584, -0.01426059, -0.03736738,\n",
      "       -0.93176889,  0.36084214,  0.99856395, -0.20397425, -2.05498862,\n",
      "       -5.90556669, -6.1014204 , -2.26402855,  3.03112411, -4.52418995,\n",
      "        2.81378031,  0.43737584, -0.76855147, -0.45171213,  0.11826754,\n",
      "        0.63191301,  1.38548589, -3.12825918, -2.94066858, -4.47891855,\n",
      "        0.26958263,  7.49476051, -1.        , -2.79795647,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406]), action=array([ 0.22192512,  0.5544285 ,  0.13489157, -0.15590574], dtype=float32), reward=[0.0], next_state=array([ 2.18140030e+00, -3.36077380e+00,  4.87722643e-02, -1.83140137e-03,\n",
      "       -6.76889066e-03, -9.59209442e-01,  2.82609701e-01,  7.94693172e-01,\n",
      "       -6.00770675e-02, -2.05927444e+00, -6.77440071e+00, -4.85299063e+00,\n",
      "       -2.47272348e+00,  2.59067535e+00, -5.03618383e+00,  2.69806433e+00,\n",
      "        3.19187462e-01, -7.93902993e-01, -5.07806540e-01,  9.98497978e-02,\n",
      "        7.09550858e-01,  1.57234550e+00, -2.85180926e+00, -2.42948890e+00,\n",
      "       -5.46813631e+00, -3.22852820e-01,  7.59366417e+00, -1.00000000e+00,\n",
      "       -2.51719403e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 2.18140030e+00, -3.36077380e+00,  4.87722643e-02, -1.83140137e-03,\n",
      "       -6.76889066e-03, -9.59209442e-01,  2.82609701e-01,  7.94693172e-01,\n",
      "       -6.00770675e-02, -2.05927444e+00, -6.77440071e+00, -4.85299063e+00,\n",
      "       -2.47272348e+00,  2.59067535e+00, -5.03618383e+00,  2.69806433e+00,\n",
      "        3.19187462e-01, -7.93902993e-01, -5.07806540e-01,  9.98497978e-02,\n",
      "        7.09550858e-01,  1.57234550e+00, -2.85180926e+00, -2.42948890e+00,\n",
      "       -5.46813631e+00, -3.22852820e-01,  7.59366417e+00, -1.00000000e+00,\n",
      "       -2.51719403e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.18478894,  0.18256527,  0.3764347 , -0.18015498], dtype=float32), reward=[0.0], next_state=array([ 1.58297348e+00, -3.67654276e+00, -1.46463603e-01,  4.12694272e-03,\n",
      "        1.94801446e-02, -9.79437232e-01,  2.00764373e-01,  6.50767267e-01,\n",
      "        5.84284849e-02, -2.12540960e+00, -7.71569347e+00, -3.63122034e+00,\n",
      "       -2.46224904e+00,  2.12808418e+00, -5.56564856e+00,  2.55085087e+00,\n",
      "        1.95929855e-01, -7.97403276e-01, -5.63752770e-01,  8.91192928e-02,\n",
      "        9.62534010e-01,  1.88021731e+00, -2.73067999e+00, -2.19135213e+00,\n",
      "       -7.08571577e+00, -8.37537706e-01,  7.68205357e+00, -1.00000000e+00,\n",
      "       -2.23294663e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 1.58297348e+00, -3.67654276e+00, -1.46463603e-01,  4.12694272e-03,\n",
      "        1.94801446e-02, -9.79437232e-01,  2.00764373e-01,  6.50767267e-01,\n",
      "        5.84284849e-02, -2.12540960e+00, -7.71569347e+00, -3.63122034e+00,\n",
      "       -2.46224904e+00,  2.12808418e+00, -5.56564856e+00,  2.55085087e+00,\n",
      "        1.95929855e-01, -7.97403276e-01, -5.63752770e-01,  8.91192928e-02,\n",
      "        9.62534010e-01,  1.88021731e+00, -2.73067999e+00, -2.19135213e+00,\n",
      "       -7.08571577e+00, -8.37537706e-01,  7.68205357e+00, -1.00000000e+00,\n",
      "       -2.23294663e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.05755114,  0.05413285,  0.49944097, -0.20536032], dtype=float32), reward=[0.0], next_state=array([ 9.26343918e-01, -3.88349700e+00, -3.09909403e-01,  4.68246825e-03,\n",
      "        3.93675715e-02, -9.92480636e-01,  1.15803719e-01,  4.46797907e-01,\n",
      "        1.53506920e-01, -2.15519738e+00, -8.36672020e+00, -2.23527789e+00,\n",
      "       -1.89373088e+00,  1.63683891e+00, -6.13082027e+00,  2.39246798e+00,\n",
      "        6.69627637e-02, -7.75951385e-01, -6.21005416e-01,  8.81354213e-02,\n",
      "        1.31245124e+00,  2.22211790e+00, -2.57914925e+00, -2.18916869e+00,\n",
      "       -9.00251484e+00, -1.38239455e+00,  7.75980759e+00, -1.00000000e+00,\n",
      "       -1.94560754e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 9.26343918e-01, -3.88349700e+00, -3.09909403e-01,  4.68246825e-03,\n",
      "        3.93675715e-02, -9.92480636e-01,  1.15803719e-01,  4.46797907e-01,\n",
      "        1.53506920e-01, -2.15519738e+00, -8.36672020e+00, -2.23527789e+00,\n",
      "       -1.89373088e+00,  1.63683891e+00, -6.13082027e+00,  2.39246798e+00,\n",
      "        6.69627637e-02, -7.75951385e-01, -6.21005416e-01,  8.81354213e-02,\n",
      "        1.31245124e+00,  2.22211790e+00, -2.57914925e+00, -2.18916869e+00,\n",
      "       -9.00251484e+00, -1.38239455e+00,  7.75980759e+00, -1.00000000e+00,\n",
      "       -1.94560754e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.03581294,  0.06315364,  0.5827522 , -0.19980763], dtype=float32), reward=[0.0], next_state=array([ 2.57352829e-01, -3.97616267e+00, -3.90325963e-01,  1.57832436e-03,\n",
      "        4.86407317e-02, -9.98303473e-01,  3.19657624e-02,  1.46568716e-01,\n",
      "        1.97533026e-01, -2.06538939e+00, -8.30199051e+00, -8.16681027e-01,\n",
      "       -6.67251170e-01,  1.13065910e+00, -6.74036217e+00,  2.24303222e+00,\n",
      "       -6.12804741e-02, -7.24934697e-01, -6.78693533e-01,  1.00447074e-01,\n",
      "        1.81357265e+00,  2.50142837e+00, -2.25722814e+00, -2.39960194e+00,\n",
      "       -1.10823412e+01, -2.15046310e+00,  7.82681751e+00, -1.00000000e+00,\n",
      "       -1.65557480e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 2.57352829e-01, -3.97616267e+00, -3.90325963e-01,  1.57832436e-03,\n",
      "        4.86407317e-02, -9.98303473e-01,  3.19657624e-02,  1.46568716e-01,\n",
      "        1.97533026e-01, -2.06538939e+00, -8.30199051e+00, -8.16681027e-01,\n",
      "       -6.67251170e-01,  1.13065910e+00, -6.74036217e+00,  2.24303222e+00,\n",
      "       -6.12804741e-02, -7.24934697e-01, -6.78693533e-01,  1.00447074e-01,\n",
      "        1.81357265e+00,  2.50142837e+00, -2.25722814e+00, -2.39960194e+00,\n",
      "       -1.10823412e+01, -2.15046310e+00,  7.82681751e+00, -1.00000000e+00,\n",
      "       -1.65557480e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.22167185,  0.1134093 ,  0.7597637 , -0.08588797], dtype=float32), reward=[0.0], next_state=array([-3.40385437e-01, -3.97261071e+00, -3.51118118e-01, -1.90400635e-03,\n",
      "        4.38103899e-02, -9.98142123e-01, -4.23018187e-02, -2.13498637e-01,\n",
      "        1.62981972e-01, -1.74344540e+00, -7.03501129e+00,  2.72562355e-01,\n",
      "        8.86972547e-01,  6.44248962e-01, -7.41536236e+00,  2.09516287e+00,\n",
      "       -1.70375034e-01, -6.37146592e-01, -7.40249991e-01,  1.30562380e-01,\n",
      "        2.73118806e+00,  2.44383788e+00, -1.56717861e+00, -2.82780075e+00,\n",
      "       -1.33950014e+01, -3.60607505e+00,  7.88299179e+00, -1.00000000e+00,\n",
      "       -1.36324990e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([-3.40385437e-01, -3.97261071e+00, -3.51118118e-01, -1.90400635e-03,\n",
      "        4.38103899e-02, -9.98142123e-01, -4.23018187e-02, -2.13498637e-01,\n",
      "        1.62981972e-01, -1.74344540e+00, -7.03501129e+00,  2.72562355e-01,\n",
      "        8.86972547e-01,  6.44248962e-01, -7.41536236e+00,  2.09516287e+00,\n",
      "       -1.70375034e-01, -6.37146592e-01, -7.40249991e-01,  1.30562380e-01,\n",
      "        2.73118806e+00,  2.44383788e+00, -1.56717861e+00, -2.82780075e+00,\n",
      "       -1.33950014e+01, -3.60607505e+00,  7.88299179e+00, -1.00000000e+00,\n",
      "       -1.36324990e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.39092174, -0.36182696,  0.7060935 ,  0.0327196 ], dtype=float32), reward=[0.0], next_state=array([-8.22126389e-01, -3.91447568e+00, -1.46044880e-01, -2.03153328e-03,\n",
      "        1.84893776e-02, -9.94534731e-01, -1.02736928e-01, -8.01274240e-01,\n",
      "        7.76088089e-02, -1.41112816e+00, -5.59389353e+00,  8.34427595e-01,\n",
      "        3.22224617e+00,  1.80351257e-01, -8.15611649e+00,  1.93660879e+00,\n",
      "       -2.33775288e-01, -4.99671519e-01, -8.12992096e-01,  1.86336622e-01,\n",
      "        4.32916498e+00,  1.96328330e+00, -5.39362729e-01, -3.77923942e+00,\n",
      "       -1.48917561e+01, -5.82889175e+00,  7.92825031e+00, -1.00000000e+00,\n",
      "       -1.06903851e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([-8.22126389e-01, -3.91447568e+00, -1.46044880e-01, -2.03153328e-03,\n",
      "        1.84893776e-02, -9.94534731e-01, -1.02736928e-01, -8.01274240e-01,\n",
      "        7.76088089e-02, -1.41112816e+00, -5.59389353e+00,  8.34427595e-01,\n",
      "        3.22224617e+00,  1.80351257e-01, -8.15611649e+00,  1.93660879e+00,\n",
      "       -2.33775288e-01, -4.99671519e-01, -8.12992096e-01,  1.86336622e-01,\n",
      "        4.32916498e+00,  1.96328330e+00, -5.39362729e-01, -3.77923942e+00,\n",
      "       -1.48917561e+01, -5.82889175e+00,  7.92825031e+00, -1.00000000e+00,\n",
      "       -1.06903851e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.65371895, -0.15071866,  0.91004133,  0.14432366], dtype=float32), reward=[0.0], next_state=array([-1.05274582e+00, -3.85766649e+00,  1.47479326e-01,  2.47162650e-03,\n",
      "       -1.88238118e-02, -9.91058826e-01, -1.32067844e-01, -8.28223228e-01,\n",
      "       -9.23698302e-03, -4.61274594e-01, -1.79431772e+00,  5.35749674e-01,\n",
      "        3.21098709e+00, -2.09299088e-01, -8.81073666e+00,  1.73841703e+00,\n",
      "       -1.81451067e-01, -3.20411086e-01, -8.98728848e-01,  2.38114864e-01,\n",
      "        5.24790859e+00, -9.12258148e-01,  1.39955550e-01, -4.97986746e+00,\n",
      "       -1.16041470e+01, -6.87176704e+00,  7.96253395e+00, -1.00000000e+00,\n",
      "       -7.73346007e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([-1.05274582e+00, -3.85766649e+00,  1.47479326e-01,  2.47162650e-03,\n",
      "       -1.88238118e-02, -9.91058826e-01, -1.32067844e-01, -8.28223228e-01,\n",
      "       -9.23698302e-03, -4.61274594e-01, -1.79431772e+00,  5.35749674e-01,\n",
      "        3.21098709e+00, -2.09299088e-01, -8.81073666e+00,  1.73841703e+00,\n",
      "       -1.81451067e-01, -3.20411086e-01, -8.98728848e-01,  2.38114864e-01,\n",
      "        5.24790859e+00, -9.12258148e-01,  1.39955550e-01, -4.97986746e+00,\n",
      "       -1.16041470e+01, -6.87176704e+00,  7.96253395e+00, -1.00000000e+00,\n",
      "       -7.73346007e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.66676533, -0.56892323,  0.6473876 , -0.16051584], dtype=float32), reward=[0.0], next_state=array([-1.08166504e+00, -3.82651114e+00,  4.47579175e-01,  7.87137449e-03,\n",
      "       -5.73544316e-02, -9.89075720e-01, -1.35564789e-01, -9.16685820e-01,\n",
      "        3.72222718e-03,  3.83819193e-02,  1.49242461e-01,  3.00633192e-01,\n",
      "        3.53524423e+00, -5.29798508e-01, -9.29859924e+00,  1.47204208e+00,\n",
      "       -5.39197847e-02, -1.30573213e-01, -9.63149428e-01,  2.28880867e-01,\n",
      "        5.26279116e+00, -2.19861436e+00, -8.74935389e-01, -5.87366533e+00,\n",
      "       -8.32035446e+00, -8.49784565e+00,  7.98579216e+00, -1.00000000e+00,\n",
      "       -4.76582825e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([-1.08166504e+00, -3.82651114e+00,  4.47579175e-01,  7.87137449e-03,\n",
      "       -5.73544316e-02, -9.89075720e-01, -1.35564789e-01, -9.16685820e-01,\n",
      "        3.72222718e-03,  3.83819193e-02,  1.49242461e-01,  3.00633192e-01,\n",
      "        3.53524423e+00, -5.29798508e-01, -9.29859924e+00,  1.47204208e+00,\n",
      "       -5.39197847e-02, -1.30573213e-01, -9.63149428e-01,  2.28880867e-01,\n",
      "        5.26279116e+00, -2.19861436e+00, -8.74935389e-01, -5.87366533e+00,\n",
      "       -8.32035446e+00, -8.49784565e+00,  7.98579216e+00, -1.00000000e+00,\n",
      "       -4.76582825e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.28790766, -0.38788643,  0.19776314, -0.15273525], dtype=float32), reward=[0.0], next_state=array([ -0.92220688,  -3.81773448,   0.76912522,   0.01148006,\n",
      "        -0.09810087,  -0.98845577,  -0.11488992,  -1.00298321,\n",
      "         0.12284055,   0.6819613 ,   2.70180249,   0.02253488,\n",
      "         3.96957231,  -0.78941917,  -9.58046722,   1.13439977,\n",
      "         0.04227583,   0.04157837,  -0.98880011,   0.13696136,\n",
      "         4.40335751,  -1.1165328 ,  -2.72541761,  -6.8361516 ,\n",
      "        -4.07372952, -10.05084038,   7.99799347,  -1.        ,\n",
      "        -0.17915979,   0.        ,   1.        ,   0.        ,\n",
      "        -0.53301406]), done=[False]), Experience(state=array([ -0.92220688,  -3.81773448,   0.76912522,   0.01148006,\n",
      "        -0.09810087,  -0.98845577,  -0.11488992,  -1.00298321,\n",
      "         0.12284055,   0.6819613 ,   2.70180249,   0.02253488,\n",
      "         3.96957231,  -0.78941917,  -9.58046722,   1.13439977,\n",
      "         0.04227583,   0.04157837,  -0.98880011,   0.13696136,\n",
      "         4.40335751,  -1.1165328 ,  -2.72541761,  -6.8361516 ,\n",
      "        -4.07372952, -10.05084038,   7.99799347,  -1.        ,\n",
      "        -0.17915979,   0.        ,   1.        ,   0.        ,\n",
      "        -0.53301406]), action=array([ 0.04690469, -0.5432479 ,  0.15394008, -0.00592754], dtype=float32), reward=[0.0], next_state=array([-5.93908310e-01, -3.83685684e+00,  9.70038176e-01,  9.13624186e-03,\n",
      "       -1.22558437e-01, -9.89694536e-01, -7.34898299e-02, -4.09877598e-01,\n",
      "        2.55567908e-01,  1.04398060e+00,  4.25931692e+00, -3.26139808e-01,\n",
      "        1.75209165e+00, -9.92856979e-01, -9.56229305e+00,  6.94861650e-01,\n",
      "        4.31994796e-02,  1.67966694e-01, -9.84845757e-01,  1.27352774e-04,\n",
      "        3.00224400e+00,  1.03126562e+00, -3.16140962e+00, -7.38146591e+00,\n",
      "        1.95937729e+00, -1.01882629e+01,  7.99912262e+00, -1.00000000e+00,\n",
      "        1.18511282e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([-5.93908310e-01, -3.83685684e+00,  9.70038176e-01,  9.13624186e-03,\n",
      "       -1.22558437e-01, -9.89694536e-01, -7.34898299e-02, -4.09877598e-01,\n",
      "        2.55567908e-01,  1.04398060e+00,  4.25931692e+00, -3.26139808e-01,\n",
      "        1.75209165e+00, -9.92856979e-01, -9.56229305e+00,  6.94861650e-01,\n",
      "        4.31994796e-02,  1.67966694e-01, -9.84845757e-01,  1.27352774e-04,\n",
      "        3.00224400e+00,  1.03126562e+00, -3.16140962e+00, -7.38146591e+00,\n",
      "        1.95937729e+00, -1.01882629e+01,  7.99912262e+00, -1.00000000e+00,\n",
      "        1.18511282e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.12110353, -0.519341  ,  0.6486006 , -0.55178773], dtype=float32), reward=[0.0], next_state=array([-2.18750000e-01, -3.88306737e+00,  9.42398310e-01,  3.17038922e-03,\n",
      "       -1.18133403e-01, -9.92624819e-01, -2.70266552e-02,  3.00622821e-01,\n",
      "        2.81740904e-01,  1.12734437e+00,  4.65869904e+00, -6.42380297e-01,\n",
      "       -1.08176923e+00, -1.19872856e+00, -9.27852058e+00,  1.38436973e-01,\n",
      "       -1.68093462e-02,  2.63352156e-01, -9.54705179e-01, -1.37481615e-01,\n",
      "        2.63647223e+00,  2.28882813e+00, -2.94445276e+00, -7.98984766e+00,\n",
      "        7.00884485e+00, -1.00063686e+01,  7.98917580e+00, -1.00000000e+00,\n",
      "        4.16018277e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([-2.18750000e-01, -3.88306737e+00,  9.42398310e-01,  3.17038922e-03,\n",
      "       -1.18133403e-01, -9.92624819e-01, -2.70266552e-02,  3.00622821e-01,\n",
      "        2.81740904e-01,  1.12734437e+00,  4.65869904e+00, -6.42380297e-01,\n",
      "       -1.08176923e+00, -1.19872856e+00, -9.27852058e+00,  1.38436973e-01,\n",
      "       -1.68093462e-02,  2.63352156e-01, -9.54705179e-01, -1.37481615e-01,\n",
      "        2.63647223e+00,  2.28882813e+00, -2.94445276e+00, -7.98984766e+00,\n",
      "        7.00884485e+00, -1.00063686e+01,  7.98917580e+00, -1.00000000e+00,\n",
      "        4.16018277e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.15728344, -0.6164316 ,  0.5756847 , -0.8020793 ], dtype=float32), reward=[0.0], next_state=array([ 8.67614746e-02, -3.94069362e+00,  6.90436959e-01, -1.01138349e-03,\n",
      "       -8.61848667e-02, -9.96220767e-01,  1.07417461e-02,  9.46397662e-01,\n",
      "        1.61361709e-01,  8.25029373e-01,  3.38049316e+00, -7.11698234e-01,\n",
      "       -3.73859382e+00, -1.46441460e+00, -8.78323936e+00, -5.03524780e-01,\n",
      "       -1.08041376e-01,  3.46526921e-01, -8.95389080e-01, -2.57923484e-01,\n",
      "        2.77258778e+00,  2.79967904e+00, -2.38467407e+00, -8.34319305e+00,\n",
      "        1.07331200e+01, -9.03074169e+00,  7.96816826e+00, -1.00000000e+00,\n",
      "        7.12949276e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False]), Experience(state=array([ 8.67614746e-02, -3.94069362e+00,  6.90436959e-01, -1.01138349e-03,\n",
      "       -8.61848667e-02, -9.96220767e-01,  1.07417461e-02,  9.46397662e-01,\n",
      "        1.61361709e-01,  8.25029373e-01,  3.38049316e+00, -7.11698234e-01,\n",
      "       -3.73859382e+00, -1.46441460e+00, -8.78323936e+00, -5.03524780e-01,\n",
      "       -1.08041376e-01,  3.46526921e-01, -8.95389080e-01, -2.57923484e-01,\n",
      "        2.77258778e+00,  2.79967904e+00, -2.38467407e+00, -8.34319305e+00,\n",
      "        1.07331200e+01, -9.03074169e+00,  7.96816826e+00, -1.00000000e+00,\n",
      "        7.12949276e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([ 0.32722408, -0.6917909 ,  0.36280915, -0.58521676], dtype=float32), reward=[0.0], next_state=array([ 2.37092972e-01, -3.98808908e+00,  2.38387644e-01, -9.19129117e-04,\n",
      "       -2.96916571e-02, -9.99123693e-01,  2.94865742e-02,  1.56097794e+00,\n",
      "        2.89887507e-02,  3.14667076e-01,  1.26909041e+00, -5.07065356e-01,\n",
      "       -6.24889946e+00, -1.82541847e+00, -8.16409492e+00, -1.21431875e+00,\n",
      "       -2.13970944e-01,  4.26005453e-01, -8.07236075e-01, -3.48002374e-01,\n",
      "        3.12926078e+00,  2.86214972e+00, -1.75537813e+00, -8.43153954e+00,\n",
      "        1.23432760e+01, -8.32137585e+00,  7.93612862e+00, -1.00000000e+00,\n",
      "        1.00889325e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False])], maxlen=1000000)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>   self.memory.memory[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience(state=array([ 0.00000000e+00, -4.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "       -0.00000000e+00, -0.00000000e+00, -4.37113883e-08,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00, -1.00000000e+01,  0.00000000e+00,\n",
      "        1.00000000e+00, -0.00000000e+00, -0.00000000e+00, -4.37113883e-08,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00, -6.30408478e+00, -1.00000000e+00,\n",
      "       -4.92529202e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), action=array([-0.13057055,  0.42653787, -0.03732488,  0.23326162], dtype=float32), reward=[0.0], next_state=array([ 1.06143951e-02, -3.99998260e+00,  7.24867499e-03,  9.99998748e-01,\n",
      "        1.32018444e-03, -1.00042257e-06,  9.01577994e-04, -3.59757915e-02,\n",
      "        5.71791061e-05,  5.27655929e-02,  2.12117672e-01,  4.92979190e-04,\n",
      "        1.44622102e-01,  2.57720947e-02, -9.99996185e+00,  8.35843850e-03,\n",
      "        9.99999106e-01,  1.20617892e-03, -2.87504690e-06, -7.16612791e-04,\n",
      "        2.85254922e-02,  1.63642791e-04,  4.82628606e-02,  5.85402846e-01,\n",
      "        1.28167751e-03,  1.25479847e-01, -6.11648560e+00, -1.00000000e+00,\n",
      "       -5.15641546e+00,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "       -5.33014059e-01]), done=[False])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>   self.memory.memory[0].reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>   w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m    [... skipping 31 hidden frame(s)]\u001b[0m\n",
      "\n",
      "None\n",
      "  \u001b[0;32m<ipython-input-10-2e4a8bc04803>\u001b[0m(9)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      5 \u001b[0m\u001b[0;31m# %autoreload 2b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      6 \u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      7 \u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      8 \u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 9 \u001b[0;31m\u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/runner.py\u001b[0m(34)\u001b[0;36mrun\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     32 \u001b[0m            \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     33 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 34 \u001b[0;31m            \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     35 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     36 \u001b[0m            \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "  \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(73)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     71 \u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m1\u001b[1;32m    72 \u001b[0m                \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 73 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     74 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m2\u001b[1;32m    75 \u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(106)\u001b[0;36mlearn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    104 \u001b[0;31m        \u001b[0;31m# ---------------------------- update critic ---------------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    105 \u001b[0;31m        \u001b[0;31m# Get predicted next-state actions and Q values from target models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 106 \u001b[0;31m        \u001b[0mactions_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    107 \u001b[0;31m        \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    108 \u001b[0;31m        \u001b[0;31m# Compute Q targets for current states (y_i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b 191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoint 3 at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:191\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(77)\u001b[0;36mact\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;31m2\u001b[0;32m    75 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     76 \u001b[0;31m        \u001b[0;34m\"\"\"Returns actions for given state as per current policy.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 77 \u001b[0;31m        \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     78 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     79 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mEPSILON_DECAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(77)\u001b[0;36mact\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;31m2\u001b[0;32m    75 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     76 \u001b[0;31m        \u001b[0;34m\"\"\"Returns actions for given state as per current policy.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 77 \u001b[0;31m        \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     78 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     79 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mEPSILON_DECAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m1\u001b[1;32m    72 \u001b[0m                \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     73 \u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     74 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m2\u001b[1;32m    75 \u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     76 \u001b[0m        \u001b[0;34m\"\"\"Returns actions for given state as per current policy.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 77 \u001b[0;31m        \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     78 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     79 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mEPSILON_DECAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     80 \u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     81 \u001b[0m            \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     82 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  np.shape(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33,)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documented commands (type help <topic>):\n",
      "========================================\n",
      "EOF    cl         disable  interact  next    psource  rv           undisplay\n",
      "a      clear      display  j         p       q        s            unt      \n",
      "alias  commands   down     jump      pdef    quit     skip_hidden  until    \n",
      "args   condition  enable   l         pdoc    r        source       up       \n",
      "b      cont       exit     list      pfile   restart  step         w        \n",
      "break  continue   h        ll        pinfo   return   tbreak       whatis   \n",
      "bt     d          help     longlist  pinfo2  retval   u            where    \n",
      "c      debug      ignore   n         pp      run      unalias    \n",
      "\n",
      "Miscellaneous help topics:\n",
      "==========================\n",
      "exec  pdb\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h bt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w(here)\n",
      "        Print a stack trace, with the most recent frame at the bottom.\n",
      "        An arrow indicates the \"current frame\", which determines the\n",
      "        context of most commands.  'bt' is an alias for this command.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h cl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl(ear) filename:lineno\n",
      "cl(ear) [bpnumber [bpnumber...]]\n",
      "        With a space separated list of breakpoint numbers, clear\n",
      "        those breakpoints.  Without argument, clear all breaks (but\n",
      "        first ask confirmation).  With a filename:lineno argument,\n",
      "        clear all breaks at that line in that file.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print lines of code from the current stack frame\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print lines of code from the current stack frame\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h EOF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOF\n",
      "        Handles the receipt of EOF as a command.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a(rgs)\n",
      "        Print the argument list of the current function.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h alias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alias [name [command [parameter parameter ...] ]]\n",
      "        Create an alias called 'name' that executes 'command'.  The\n",
      "        command must *not* be enclosed in quotes.  Replaceable\n",
      "        parameters can be indicated by %1, %2, and so on, while %* is\n",
      "        replaced by all the parameters.  If no command is given, the\n",
      "        current alias for name is shown. If no name is given, all\n",
      "        aliases are listed.\n",
      "\n",
      "        Aliases may be nested and can contain anything that can be\n",
      "        legally typed at the pdb prompt.  Note!  You *can* override\n",
      "        internal pdb commands with aliases!  Those internal commands\n",
      "        are then hidden until the alias is removed.  Aliasing is\n",
      "        recursively applied to the first word of the command line; all\n",
      "        other words in the line are left alone.\n",
      "\n",
      "        As an example, here are two useful aliases (especially when\n",
      "        placed in the .pdbrc file):\n",
      "\n",
      "        # Print instance variables (usage \"pi classInst\")\n",
      "        alias pi for k in %1.__dict__.keys(): print(\"%1.\",k,\"=\",%1.__dict__[k])\n",
      "        # Print instance variables in self\n",
      "        alias ps pi self\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documented commands (type help <topic>):\n",
      "========================================\n",
      "EOF    cl         disable  interact  next    psource  rv           undisplay\n",
      "a      clear      display  j         p       q        s            unt      \n",
      "alias  commands   down     jump      pdef    quit     skip_hidden  until    \n",
      "args   condition  enable   l         pdoc    r        source       up       \n",
      "b      cont       exit     list      pfile   restart  step         w        \n",
      "break  continue   h        ll        pinfo   return   tbreak       whatis   \n",
      "bt     d          help     longlist  pinfo2  retval   u            where    \n",
      "c      debug      ignore   n         pp      run      unalias    \n",
      "\n",
      "Miscellaneous help topics:\n",
      "==========================\n",
      "exec  pdb\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h break\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b(reak) [ ([filename:]lineno | function) [, condition] ]\n",
      "        Without argument, list all breaks.\n",
      "\n",
      "        With a line number argument, set a break at this line in the\n",
      "        current file.  With a function name, set a break at the first\n",
      "        executable line of that function.  If a second argument is\n",
      "        present, it is a string specifying an expression which must\n",
      "        evaluate to true before the breakpoint is honored.\n",
      "\n",
      "        The line number may be prefixed with a filename and a colon,\n",
      "        to specify a breakpoint in another file (probably one that\n",
      "        hasn't been loaded yet).  The file is searched for on\n",
      "        sys.path; the .py suffix may be omitted.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h whatis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whatis arg\n",
      "        Print the type of the argument.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u(p) [count]\n",
      "        Move the current frame count (default one) levels up in the\n",
      "        stack trace (to an older frame).\n",
      "\n",
      "        Will skip hidden frames.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h unt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unt(il) [lineno]\n",
      "        Without argument, continue execution until the line with a\n",
      "        number greater than the current one is reached.  With a line\n",
      "        number, continue execution until a line with a number greater\n",
      "        or equal to that is reached.  In both cases, also stop when\n",
      "        the current frame returns.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documented commands (type help <topic>):\n",
      "========================================\n",
      "EOF    cl         disable  interact  next    psource  rv           undisplay\n",
      "a      clear      display  j         p       q        s            unt      \n",
      "alias  commands   down     jump      pdef    quit     skip_hidden  until    \n",
      "args   condition  enable   l         pdoc    r        source       up       \n",
      "b      cont       exit     list      pfile   restart  step         w        \n",
      "break  continue   h        ll        pinfo   return   tbreak       whatis   \n",
      "bt     d          help     longlist  pinfo2  retval   u            where    \n",
      "c      debug      ignore   n         pp      run      unalias    \n",
      "\n",
      "Miscellaneous help topics:\n",
      "==========================\n",
      "exec  pdb\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h retval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retval\n",
      "        Print the return value for the last return of a function.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  htbreak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'htbreak' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h tbreak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tbreak [ ([filename:]lineno | function) [, condition] ]\n",
      "        Same arguments as break, but sets a temporary breakpoint: it\n",
      "        is automatically deleted when first hit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d(own) [count]\n",
      "        Move the current frame count (default one) levels down in the\n",
      "        stack trace (to a newer frame).\n",
      "\n",
      "        Will skip hidden frames.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h debug\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug code\n",
      "        Enter a recursive debugger that steps through the code\n",
      "        argument (which is an arbitrary expression or statement to be\n",
      "        executed in the current environment).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h j\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j(ump) lineno\n",
      "        Set the next line that will be executed.  Only available in\n",
      "        the bottom-most frame.  This lets you jump back and execute\n",
      "        code again, or jump forward to skip code that you don't want\n",
      "        to run.\n",
      "\n",
      "        It should be noted that not all jumps are allowed -- for\n",
      "        instance it is not possible to jump into the middle of a\n",
      "        for loop or out of a finally clause.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h h \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h(elp)\n",
      "        Without argument, print the list of available commands.\n",
      "        With a command name as argument, print help about that command.\n",
      "        \"help pdb\" shows the full pdb documentation.\n",
      "        \"help exec\" gives help on the ! command.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u(p) [count]\n",
      "        Move the current frame count (default one) levels up in the\n",
      "        stack trace (to an older frame).\n",
      "\n",
      "        Will skip hidden frames.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documented commands (type help <topic>):\n",
      "========================================\n",
      "EOF    cl         disable  interact  next    psource  rv           undisplay\n",
      "a      clear      display  j         p       q        s            unt      \n",
      "alias  commands   down     jump      pdef    quit     skip_hidden  until    \n",
      "args   condition  enable   l         pdoc    r        source       up       \n",
      "b      cont       exit     list      pfile   restart  step         w        \n",
      "break  continue   h        ll        pinfo   return   tbreak       whatis   \n",
      "bt     d          help     longlist  pinfo2  retval   u            where    \n",
      "c      debug      ignore   n         pp      run      unalias    \n",
      "\n",
      "Miscellaneous help topics:\n",
      "==========================\n",
      "exec  pdb\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignore bpnumber [count]\n",
      "        Set the ignore count for the given breakpoint number.  If\n",
      "        count is omitted, the ignore count is set to 0.  A breakpoint\n",
      "        becomes active when the ignore count is zero.  When non-zero,\n",
      "        the count is decremented each time the breakpoint is reached\n",
      "        and the breakpoint is not disabled and any associated\n",
      "        condition evaluates to true.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h cl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl(ear) filename:lineno\n",
      "cl(ear) [bpnumber [bpnumber...]]\n",
      "        With a space separated list of breakpoint numbers, clear\n",
      "        those breakpoints.  Without argument, clear all breaks (but\n",
      "        first ask confirmation).  With a filename:lineno argument,\n",
      "        clear all breaks at that line in that file.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h rv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retval\n",
      "        Print the return value for the last return of a function.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h psource\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print (or run through pager) the source code for an object.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h pp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pp expression\n",
      "        Pretty-print the value of the expression.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h pdef\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the call signature for any callable object.\n",
      "\n",
      "        The debugger interface to %pdef\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h ll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print lines of code from the current stack frame.\n",
      "\n",
      "        Shows more lines than 'list' does.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documented commands (type help <topic>):\n",
      "========================================\n",
      "EOF    cl         disable  interact  next    psource  rv           undisplay\n",
      "a      clear      display  j         p       q        s            unt      \n",
      "alias  commands   down     jump      pdef    quit     skip_hidden  until    \n",
      "args   condition  enable   l         pdoc    r        source       up       \n",
      "b      cont       exit     list      pfile   restart  step         w        \n",
      "break  continue   h        ll        pinfo   return   tbreak       whatis   \n",
      "bt     d          help     longlist  pinfo2  retval   u            where    \n",
      "c      debug      ignore   n         pp      run      unalias    \n",
      "\n",
      "Miscellaneous help topics:\n",
      "==========================\n",
      "exec  pdb\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h display\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display [expression]\n",
      "\n",
      "        Display the value of the expression if it changed, each time execution\n",
      "        stops in the current frame.\n",
      "\n",
      "        Without expression, list all display expressions for the current frame.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h disable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disable bpnumber [bpnumber ...]\n",
      "        Disables the breakpoints given as a space separated list of\n",
      "        breakpoint numbers.  Disabling a breakpoint means it cannot\n",
      "        cause the program to stop execution, but unlike clearing a\n",
      "        breakpoint, it remains in the list of breakpoints and can be\n",
      "        (re-)enabled.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h condition\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition bpnumber [condition]\n",
      "        Set a new condition for the breakpoint, an expression which\n",
      "        must evaluate to true before the breakpoint is honored.  If\n",
      "        condition is absent, any existing condition is removed; i.e.,\n",
      "        the breakpoint is made unconditional.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  h break\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b(reak) [ ([filename:]lineno | function) [, condition] ]\n",
      "        Without argument, list all breaks.\n",
      "\n",
      "        With a line number argument, set a break at this line in the\n",
      "        current file.  With a function name, set a break at the first\n",
      "        executable line of that function.  If a second argument is\n",
      "        present, it is a string specifying an expression which must\n",
      "        evaluate to true before the breakpoint is honored.\n",
      "\n",
      "        The line number may be prefixed with a filename and a colon,\n",
      "        to specify a breakpoint in another file (probably one that\n",
      "        hasn't been loaded yet).  The file is searched for on\n",
      "        sys.path; the .py suffix may be omitted.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Type         Disp Enb   Where\n",
      "1   breakpoint   keep yes   at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:72\n",
      "\tbreakpoint already hit 1 time\n",
      "2   breakpoint   keep yes   at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:75\n",
      "\tbreakpoint already hit 2 times\n",
      "3   breakpoint   keep yes   at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:191\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  clear 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted breakpoint 2 at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:75\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Type         Disp Enb   Where\n",
      "1   breakpoint   keep yes   at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:72\n",
      "\tbreakpoint already hit 1 time\n",
      "3   breakpoint   keep yes   at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:191\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(72)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     70 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     71 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m1\u001b[0;32m--> 72 \u001b[0;31m                \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     73 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(187)\u001b[0;36msample\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    185 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    186 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 187 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    188 \u001b[0;31m        \u001b[0;34m\"\"\"Randomly sample a batch of experiences from memory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(189)\u001b[0;36msample\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    187 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    188 \u001b[0;31m        \u001b[0;34m\"\"\"Randomly sample a batch of experiences from memory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m3\u001b[0;32m   191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(191)\u001b[0;36msample\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    192 \u001b[0;31m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    193 \u001b[0;31m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p experiences[0].reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p experiences[0].state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 3.74260521,  1.30481577,  0.72065175,  0.21013209,  0.14361505,\n",
      "       -0.54456174,  0.79916942, -2.99421239,  0.59926176, -0.92209125,\n",
      "        1.71133161, -0.89367723, -6.1378274 ,  7.62707138, -0.54108763,\n",
      "        1.25264263, -0.72786838, -0.13838984,  0.66001654, -0.12423407,\n",
      "       -4.16871071,  1.64133048, -0.20037714,  1.66415811, -3.14168978,\n",
      "        3.82270241,  3.34989738, -1.        , -7.26486015,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p experiences[0].action\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-0.23741655, -0.43217555,  0.04944853, -0.34994152], dtype=float32)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m    186 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    187 \u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    188 \u001b[0m        \u001b[0;34m\"\"\"Randomly sample a batch of experiences from memory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    189 \u001b[0m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    190 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    192 \u001b[0m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    193 \u001b[0m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    194 \u001b[0m        \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    195 \u001b[0m        \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    196 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p experiences[0].next_states\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** AttributeError: 'Experience' object has no attribute 'next_states'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p experiences[0].next_state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 3.86730766,  1.10121477,  0.23466928,  0.0877711 ,  0.06690416,\n",
      "       -0.59871387,  0.79332328, -4.0011344 ,  0.37668628, -1.19627059,\n",
      "        1.50833106, -3.17391753, -6.04429054,  7.76144505, -0.82129276,\n",
      "        1.17817092, -0.65274692, -0.01859114,  0.72584504, -0.21615945,\n",
      "       -4.15250683,  1.84039509, -0.79419285,  0.94869405, -3.19777179,\n",
      "        3.66718698,  3.61785316, -1.        , -7.13520432,  0.        ,\n",
      "        1.        ,  0.        , -0.53301406])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  np.shape(p experiences[0].next_state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SyntaxError: invalid syntax\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p np.shape(experiences[0].next_state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33,)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b 75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoint 4 at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:75\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(191)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    192 \u001b[0;31m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    193 \u001b[0;31m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(191)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    192 \u001b[0;31m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    193 \u001b[0;31m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m    [... skipping 31 hidden frame(s)]\u001b[0m\n",
      "\n",
      "None\n",
      "  \u001b[0;32m<ipython-input-10-2e4a8bc04803>\u001b[0m(9)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      5 \u001b[0m\u001b[0;31m# %autoreload 2b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      6 \u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      7 \u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      8 \u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 9 \u001b[0;31m\u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/runner.py\u001b[0m(34)\u001b[0;36mrun\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     32 \u001b[0m            \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     33 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 34 \u001b[0;31m            \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     35 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     36 \u001b[0m            \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "  \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(72)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     70 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     71 \u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m1\u001b[0;32m--> 72 \u001b[0;31m                \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     73 \u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     74 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "  \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(191)\u001b[0;36msample\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m    189 \u001b[0m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    190 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    192 \u001b[0m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    193 \u001b[0m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(191)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    192 \u001b[0;31m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    193 \u001b[0;31m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(191)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    192 \u001b[0;31m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    193 \u001b[0;31m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(191)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    192 \u001b[0;31m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    193 \u001b[0;31m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(191)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    192 \u001b[0;31m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    193 \u001b[0;31m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(191)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    192 \u001b[0;31m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    193 \u001b[0;31m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  disable 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabled breakpoint 3 at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:191\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(191)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    192 \u001b[0;31m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    193 \u001b[0;31m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(191)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    192 \u001b[0;31m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    193 \u001b[0;31m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Type         Disp Enb   Where\n",
      "1   breakpoint   keep yes   at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:72\n",
      "\tbreakpoint already hit 2 times\n",
      "3   breakpoint   keep no    at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:191\n",
      "\tbreakpoint already hit 3 times\n",
      "4   breakpoint   keep yes   at /home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:75\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(191)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    189 \u001b[0;31m        \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;31m3\u001b[0;32m-> 191 \u001b[0;31m        \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    192 \u001b[0;31m        \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    193 \u001b[0;31m        \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(72)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     70 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     71 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m1\u001b[0;32m--> 72 \u001b[0;31m                \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     73 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m    [... skipping 31 hidden frame(s)]\u001b[0m\n",
      "\n",
      "None\n",
      "  \u001b[0;32m<ipython-input-10-2e4a8bc04803>\u001b[0m(9)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      5 \u001b[0m\u001b[0;31m# %autoreload 2b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      6 \u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      7 \u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      8 \u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 9 \u001b[0;31m\u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/runner.py\u001b[0m(34)\u001b[0;36mrun\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     32 \u001b[0m            \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     33 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 34 \u001b[0;31m            \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     35 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     36 \u001b[0m            \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(72)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     70 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     71 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m1\u001b[0;32m--> 72 \u001b[0;31m                \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     73 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m     67 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mUPDATE_EVERY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     68 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     69 \u001b[0m            \u001b[0;31m# Learn, if enough samples are available in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     70 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     71 \u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m1\u001b[0;32m--> 72 \u001b[0;31m                \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     73 \u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     74 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m4\u001b[1;32m    75 \u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     76 \u001b[0m        \u001b[0;34m\"\"\"Returns actions for given state as per current policy.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     77 \u001b[0m        \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  ll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m     62 \u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     63 \u001b[0m        \u001b[0;34m\"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     64 \u001b[0m        \u001b[0;31m# Save experience / reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     65 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     66 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     67 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mUPDATE_EVERY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     68 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     69 \u001b[0m            \u001b[0;31m# Learn, if enough samples are available in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     70 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     71 \u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m1\u001b[0;32m--> 72 \u001b[0;31m                \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     73 \u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     74 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  np.shape(next_state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33,)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p np.shape(experiences[0].next_state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'experiences' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jmh/udacity_rl/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py\u001b[0m(73)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     71 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m1\u001b[0;32m    72 \u001b[0;31m                \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 73 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;31m4\u001b[0;32m    75 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p np.shape(experiences[0].next_state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** AttributeError: 'Tensor' object has no attribute 'next_state'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  type(experiences)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>   p np.shape(experiences.next_state[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** AttributeError: 'tuple' object has no attribute 'next_state'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>   p np.shape(experiences[3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 33])\n"
     ]
    }
   ],
   "source": [
    "import runner\n",
    "import ddpg_agent\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2b\n",
    "from IPython.core.debugger import set_trace\n",
    "set_trace()\n",
    "agent = ddpg_agent.Agent(state_size=state_size, action_size=action_size, random_seed=1)\n",
    "scores = runner.run(env, agent, brain_name=brain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
